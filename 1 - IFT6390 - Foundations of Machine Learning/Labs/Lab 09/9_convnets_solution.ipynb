{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_convnets_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvGeGKRUe3cv",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks (CNN)\n",
        "\n",
        "Welcome to the wonderworld of CNNs™. If you followed the last lab session on pytorch, you should already be comfortable with Pytorch's autograd mechanism, and with the standard way to train a model. Today we will focus on convolutional neural nets, aka CNN, aka convnets. On the way, we will also get more familiar with the data loading mechanism from pytorch and the training loop. You have two goals:\n",
        "\n",
        "1. Overfit a small subset of MNIST. This is a fast and reliable way to test that a model is not garbage.\n",
        "2. reach the best possible test score on MNIST. Bon courage.\n",
        "\n",
        "# Réseaux de Neurones Convolutionnels (RNC)\n",
        "###### C'est un blague, personne n'utilise cet acronyme.\n",
        "\n",
        "Bienvenu dans le monde merveilleux des CNNs™. Si vous avez suivi la dernière démonstration sur pytorch, vous devriez déjà être à l'aise avec le mécanisme de différentiation automatique, ainsi qu'avec la méthode standard pour entraîner un modèle. Aujourd'hui,  on va s'intéresser à toutes les subtilités des réseaux de neurones convolutionnels. Au passage on va revoir le mécanisme de chargement des données et la boucle d'entraînement. Vous avez deux buts:\n",
        "\n",
        "1. mémoriser une petite fraction de MNIST avec un modèle. C'est une méthode simple et efficace pour vérifier qu'un modèle est décent.\n",
        "2. atteindre le meilleur score possible sur MNIST entier. Good luck.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhfFTl1pe3cw",
        "colab_type": "text"
      },
      "source": [
        "## Set up / Préparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20cHv52zfXT8",
        "colab_type": "code",
        "outputId": "7d148130-f741-4072-bebd-3e2667c2bd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31m  ERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31mERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ07Jtthe3cy",
        "colab_type": "code",
        "outputId": "6e6a9123-acc1-4247-e2aa-da46a828d163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(f\"Your version of Pytorch is {torch.__version__}. You should use a version >0.4.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your version of Pytorch is 1.3.1. You should use a version >0.4.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JizO0m1Ze3c6",
        "colab_type": "code",
        "outputId": "50c26691-5d36-4f32-af0a-c6513cc77bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If a GPU is available, use it\n",
        "# Pytorch uses an elegant way to keep the code device agnostic\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoIKi7ebe3c9",
        "colab_type": "text"
      },
      "source": [
        "## Data / Données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK9fKqhoe3c9",
        "colab_type": "code",
        "outputId": "9ac1c0b3-1910-4ddd-d4f1-623b04952503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# dataset\n",
        "\n",
        "train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       # Standardize with mean and std computed on train set\n",
        "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                   ]))\n",
        "\n",
        "test_data = datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3974755.37it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 58538.47it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 969296.52it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21994.17it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lknHPBxSe3dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of the scratch training set\n",
        "n_scratch = 64\n",
        "\n",
        "# This parameter influences optimization\n",
        "batch_size = 64\n",
        "# This is just for evaluation, we want is as big as the GPU can support\n",
        "batch_size_eval = 512\n",
        "\n",
        "\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3AIrdce3dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataLoaders\n",
        "\n",
        "# This is the subset of MNIST we want to overfit\n",
        "scratch_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    # The sampler is an easy way to say that we're using the elements\n",
        "    # `indices[:n_scratch]` for this loader\n",
        "    sampler=SubsetRandomSampler(indices[:n_scratch]),\n",
        "    num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "# TODO: define a train, valid and test loader\n",
        "# size of the validation set\n",
        "n_valid = 15000\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[n_valid:]),\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    sampler=SubsetRandomSampler(indices[:n_valid]),\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "# Extra Solution with `torch.utils.data.random_split`\n",
        "#n_total = len(train_data)\n",
        "#n_train = n_total - n_valid\n",
        "#train_data, valid_data = torch.utils.data.random_split(train_data, n_train, n_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFJcFGi6e3dG",
        "colab_type": "code",
        "outputId": "2c861df7-ef64-4d75-ee54-94aecdeeefc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# visualize and understand the data\n",
        "for inputs, targets in scratch_loader:\n",
        "    print(f\"This is the shape of one batch {inputs.shape}. What is the meaning of each dimension?  batch size * channels * height * width\")\n",
        "    print(\"target\", targets.shape, set(targets))\n",
        "    img = inputs[0,0]\n",
        "    plt.imshow(img, cmap='Greys_r')\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the shape of one batch torch.Size([64, 1, 28, 28]). What is the meaning of each dimension?  batch size * channels * height * width\n",
            "target torch.Size([64]) {tensor(9), tensor(6), tensor(6), tensor(9), tensor(5), tensor(4), tensor(3), tensor(7), tensor(5), tensor(4), tensor(6), tensor(9), tensor(9), tensor(5), tensor(5), tensor(6), tensor(7), tensor(4), tensor(4), tensor(6), tensor(5), tensor(8), tensor(2), tensor(0), tensor(6), tensor(7), tensor(2), tensor(3), tensor(9), tensor(9), tensor(2), tensor(7), tensor(9), tensor(7), tensor(1), tensor(4), tensor(3), tensor(9), tensor(9), tensor(6), tensor(2), tensor(9), tensor(6), tensor(7), tensor(2), tensor(6), tensor(7), tensor(2), tensor(7), tensor(7), tensor(1), tensor(7), tensor(8), tensor(7), tensor(0), tensor(2), tensor(5), tensor(2), tensor(2), tensor(6), tensor(5), tensor(4), tensor(5), tensor(0)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4lapoWe3dL",
        "colab_type": "text"
      },
      "source": [
        "## Models / Modèles\n",
        "You can find below a basic CNN. You will have to define your own model a bit later. First let's try to train this one!\n",
        "\n",
        "Vous avez ci-dessous un CNN élémentaire. Vous devrez définir votre propre modèle un peu plus tard. Pour l'instant essayons déjà d'entraîner celui-ci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30OitDs4e3dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicNet(nn.Module):\n",
        "    \"\"\"Affordable convolutions for the people.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        #self.bn = nn.BatchNorm2d(64)\n",
        "        self.fc = nn.Linear(64*7*7, 10)\n",
        "\n",
        "    def forward(self, xin):\n",
        "        # xin is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = F.relu(self.conv1(xin)) # + xin\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # x is [bs, 32, 14, 14]\n",
        "        # x = self.bn(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # x is [bs, 64, 7, 7]\n",
        "\n",
        "        flatten_size = 64*7*7\n",
        "        x = x.view(-1, flatten_size ) # flatten\n",
        "        x = F.relu(self.fc(x))\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrCD4Mjfe3dO",
        "colab_type": "text"
      },
      "source": [
        "## Training / Entraînement\n",
        "\n",
        "You have to define general training and testing loops that can be applied to any pytorch module.\n",
        "\n",
        "Vous devez définir des boucles d'entraînement et de test générales qui s'appliquent à n'importe quel module pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agd2qklAe3dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Surrogate loss used for training\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# spot to save your learning curves, and potentially checkpoint your models\n",
        "savedir = 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d6q1p3Ee3dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch ):\n",
        "    \"\"\"Perform one epoch of training.\"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "        \n",
        "        # Let them code what's here\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ###\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls48bZZGe3dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, target in test_loader:\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            \n",
        "            # TODO: code the evaluation loop\n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item() # sum up batch loss\n",
        "            # output = batch size * n_classes\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            # pred = output.max(1, keepdim=True)\n",
        "            # pred = pred[1] # get the index of the max log-probability\n",
        "\n",
        "            # correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            # #\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_size,\n",
        "        100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTy5iKHe3dY",
        "colab_type": "code",
        "outputId": "402b54df-8050-4b1f-f4b3-29029822eb3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BasicNet().to(device)\n",
        "\n",
        "lr = 0.0005\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train(model, scratch_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, scratch_loader)\n",
        "    \n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/64 (0%)]\tLoss: 2.303838\n",
            "\n",
            "Test set: Average loss: 2.3019, Accuracy: 5/64 (8%)\n",
            "\n",
            "Train Epoch: 2 [0/64 (0%)]\tLoss: 2.301873\n",
            "\n",
            "Test set: Average loss: 2.2980, Accuracy: 6/64 (9%)\n",
            "\n",
            "Train Epoch: 3 [0/64 (0%)]\tLoss: 2.298041\n",
            "\n",
            "Test set: Average loss: 2.2923, Accuracy: 7/64 (11%)\n",
            "\n",
            "Train Epoch: 4 [0/64 (0%)]\tLoss: 2.292341\n",
            "\n",
            "Test set: Average loss: 2.2850, Accuracy: 7/64 (11%)\n",
            "\n",
            "Train Epoch: 5 [0/64 (0%)]\tLoss: 2.284964\n",
            "\n",
            "Test set: Average loss: 2.2763, Accuracy: 8/64 (12%)\n",
            "\n",
            "Train Epoch: 6 [0/64 (0%)]\tLoss: 2.276262\n",
            "\n",
            "Test set: Average loss: 2.2670, Accuracy: 14/64 (22%)\n",
            "\n",
            "Train Epoch: 7 [0/64 (0%)]\tLoss: 2.267013\n",
            "\n",
            "Test set: Average loss: 2.2577, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 8 [0/64 (0%)]\tLoss: 2.257726\n",
            "\n",
            "Test set: Average loss: 2.2478, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 9 [0/64 (0%)]\tLoss: 2.247813\n",
            "\n",
            "Test set: Average loss: 2.2376, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 10 [0/64 (0%)]\tLoss: 2.237566\n",
            "\n",
            "Test set: Average loss: 2.2269, Accuracy: 19/64 (30%)\n",
            "\n",
            "Train Epoch: 11 [0/64 (0%)]\tLoss: 2.226946\n",
            "\n",
            "Test set: Average loss: 2.2159, Accuracy: 19/64 (30%)\n",
            "\n",
            "Train Epoch: 12 [0/64 (0%)]\tLoss: 2.215934\n",
            "\n",
            "Test set: Average loss: 2.2044, Accuracy: 17/64 (27%)\n",
            "\n",
            "Train Epoch: 13 [0/64 (0%)]\tLoss: 2.204358\n",
            "\n",
            "Test set: Average loss: 2.1923, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 14 [0/64 (0%)]\tLoss: 2.192274\n",
            "\n",
            "Test set: Average loss: 2.1794, Accuracy: 17/64 (27%)\n",
            "\n",
            "Train Epoch: 15 [0/64 (0%)]\tLoss: 2.179446\n",
            "\n",
            "Test set: Average loss: 2.1660, Accuracy: 17/64 (27%)\n",
            "\n",
            "Train Epoch: 16 [0/64 (0%)]\tLoss: 2.165982\n",
            "\n",
            "Test set: Average loss: 2.1520, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 17 [0/64 (0%)]\tLoss: 2.152007\n",
            "\n",
            "Test set: Average loss: 2.1377, Accuracy: 18/64 (28%)\n",
            "\n",
            "Train Epoch: 18 [0/64 (0%)]\tLoss: 2.137701\n",
            "\n",
            "Test set: Average loss: 2.1234, Accuracy: 20/64 (31%)\n",
            "\n",
            "Train Epoch: 19 [0/64 (0%)]\tLoss: 2.123364\n",
            "\n",
            "Test set: Average loss: 2.1090, Accuracy: 21/64 (33%)\n",
            "\n",
            "Train Epoch: 20 [0/64 (0%)]\tLoss: 2.108980\n",
            "\n",
            "Test set: Average loss: 2.0944, Accuracy: 24/64 (38%)\n",
            "\n",
            "Train Epoch: 21 [0/64 (0%)]\tLoss: 2.094388\n",
            "\n",
            "Test set: Average loss: 2.0796, Accuracy: 24/64 (38%)\n",
            "\n",
            "Train Epoch: 22 [0/64 (0%)]\tLoss: 2.079584\n",
            "\n",
            "Test set: Average loss: 2.0646, Accuracy: 25/64 (39%)\n",
            "\n",
            "Train Epoch: 23 [0/64 (0%)]\tLoss: 2.064640\n",
            "\n",
            "Test set: Average loss: 2.0495, Accuracy: 27/64 (42%)\n",
            "\n",
            "Train Epoch: 24 [0/64 (0%)]\tLoss: 2.049490\n",
            "\n",
            "Test set: Average loss: 2.0342, Accuracy: 28/64 (44%)\n",
            "\n",
            "Train Epoch: 25 [0/64 (0%)]\tLoss: 2.034151\n",
            "\n",
            "Test set: Average loss: 2.0185, Accuracy: 30/64 (47%)\n",
            "\n",
            "Train Epoch: 26 [0/64 (0%)]\tLoss: 2.018481\n",
            "\n",
            "Test set: Average loss: 2.0025, Accuracy: 30/64 (47%)\n",
            "\n",
            "Train Epoch: 27 [0/64 (0%)]\tLoss: 2.002527\n",
            "\n",
            "Test set: Average loss: 1.9864, Accuracy: 32/64 (50%)\n",
            "\n",
            "Train Epoch: 28 [0/64 (0%)]\tLoss: 1.986382\n",
            "\n",
            "Test set: Average loss: 1.9701, Accuracy: 35/64 (55%)\n",
            "\n",
            "Train Epoch: 29 [0/64 (0%)]\tLoss: 1.970065\n",
            "\n",
            "Test set: Average loss: 1.9536, Accuracy: 36/64 (56%)\n",
            "\n",
            "Train Epoch: 30 [0/64 (0%)]\tLoss: 1.953568\n",
            "\n",
            "Test set: Average loss: 1.9369, Accuracy: 37/64 (58%)\n",
            "\n",
            "Train Epoch: 31 [0/64 (0%)]\tLoss: 1.936885\n",
            "\n",
            "Test set: Average loss: 1.9200, Accuracy: 39/64 (61%)\n",
            "\n",
            "Train Epoch: 32 [0/64 (0%)]\tLoss: 1.920001\n",
            "\n",
            "Test set: Average loss: 1.9026, Accuracy: 39/64 (61%)\n",
            "\n",
            "Train Epoch: 33 [0/64 (0%)]\tLoss: 1.902649\n",
            "\n",
            "Test set: Average loss: 1.8851, Accuracy: 40/64 (62%)\n",
            "\n",
            "Train Epoch: 34 [0/64 (0%)]\tLoss: 1.885116\n",
            "\n",
            "Test set: Average loss: 1.8674, Accuracy: 44/64 (69%)\n",
            "\n",
            "Train Epoch: 35 [0/64 (0%)]\tLoss: 1.867377\n",
            "\n",
            "Test set: Average loss: 1.8494, Accuracy: 44/64 (69%)\n",
            "\n",
            "Train Epoch: 36 [0/64 (0%)]\tLoss: 1.849439\n",
            "\n",
            "Test set: Average loss: 1.8313, Accuracy: 44/64 (69%)\n",
            "\n",
            "Train Epoch: 37 [0/64 (0%)]\tLoss: 1.831337\n",
            "\n",
            "Test set: Average loss: 1.8130, Accuracy: 44/64 (69%)\n",
            "\n",
            "Train Epoch: 38 [0/64 (0%)]\tLoss: 1.813037\n",
            "\n",
            "Test set: Average loss: 1.7946, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 39 [0/64 (0%)]\tLoss: 1.794555\n",
            "\n",
            "Test set: Average loss: 1.7759, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 40 [0/64 (0%)]\tLoss: 1.775885\n",
            "\n",
            "Test set: Average loss: 1.7570, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 41 [0/64 (0%)]\tLoss: 1.757044\n",
            "\n",
            "Test set: Average loss: 1.7381, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 42 [0/64 (0%)]\tLoss: 1.738054\n",
            "\n",
            "Test set: Average loss: 1.7189, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 43 [0/64 (0%)]\tLoss: 1.718881\n",
            "\n",
            "Test set: Average loss: 1.6996, Accuracy: 45/64 (70%)\n",
            "\n",
            "Train Epoch: 44 [0/64 (0%)]\tLoss: 1.699556\n",
            "\n",
            "Test set: Average loss: 1.6801, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 45 [0/64 (0%)]\tLoss: 1.680080\n",
            "\n",
            "Test set: Average loss: 1.6605, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 46 [0/64 (0%)]\tLoss: 1.660491\n",
            "\n",
            "Test set: Average loss: 1.6408, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 47 [0/64 (0%)]\tLoss: 1.640789\n",
            "\n",
            "Test set: Average loss: 1.6210, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 48 [0/64 (0%)]\tLoss: 1.620970\n",
            "\n",
            "Test set: Average loss: 1.6010, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 49 [0/64 (0%)]\tLoss: 1.601041\n",
            "\n",
            "Test set: Average loss: 1.5810, Accuracy: 46/64 (72%)\n",
            "\n",
            "Train Epoch: 50 [0/64 (0%)]\tLoss: 1.580983\n",
            "\n",
            "Test set: Average loss: 1.5608, Accuracy: 47/64 (73%)\n",
            "\n",
            "Train Epoch: 51 [0/64 (0%)]\tLoss: 1.560827\n",
            "\n",
            "Test set: Average loss: 1.5406, Accuracy: 47/64 (73%)\n",
            "\n",
            "Train Epoch: 52 [0/64 (0%)]\tLoss: 1.540599\n",
            "\n",
            "Test set: Average loss: 1.5203, Accuracy: 47/64 (73%)\n",
            "\n",
            "Train Epoch: 53 [0/64 (0%)]\tLoss: 1.520308\n",
            "\n",
            "Test set: Average loss: 1.5000, Accuracy: 47/64 (73%)\n",
            "\n",
            "Train Epoch: 54 [0/64 (0%)]\tLoss: 1.499953\n",
            "\n",
            "Test set: Average loss: 1.4796, Accuracy: 48/64 (75%)\n",
            "\n",
            "Train Epoch: 55 [0/64 (0%)]\tLoss: 1.479581\n",
            "\n",
            "Test set: Average loss: 1.4592, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 56 [0/64 (0%)]\tLoss: 1.459209\n",
            "\n",
            "Test set: Average loss: 1.4388, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 57 [0/64 (0%)]\tLoss: 1.438833\n",
            "\n",
            "Test set: Average loss: 1.4185, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 58 [0/64 (0%)]\tLoss: 1.418477\n",
            "\n",
            "Test set: Average loss: 1.3982, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 59 [0/64 (0%)]\tLoss: 1.398151\n",
            "\n",
            "Test set: Average loss: 1.3779, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 60 [0/64 (0%)]\tLoss: 1.377869\n",
            "\n",
            "Test set: Average loss: 1.3577, Accuracy: 49/64 (77%)\n",
            "\n",
            "Train Epoch: 61 [0/64 (0%)]\tLoss: 1.357698\n",
            "\n",
            "Test set: Average loss: 1.3376, Accuracy: 50/64 (78%)\n",
            "\n",
            "Train Epoch: 62 [0/64 (0%)]\tLoss: 1.337639\n",
            "\n",
            "Test set: Average loss: 1.3177, Accuracy: 50/64 (78%)\n",
            "\n",
            "Train Epoch: 63 [0/64 (0%)]\tLoss: 1.317696\n",
            "\n",
            "Test set: Average loss: 1.2979, Accuracy: 50/64 (78%)\n",
            "\n",
            "Train Epoch: 64 [0/64 (0%)]\tLoss: 1.297879\n",
            "\n",
            "Test set: Average loss: 1.2782, Accuracy: 50/64 (78%)\n",
            "\n",
            "Train Epoch: 65 [0/64 (0%)]\tLoss: 1.278200\n",
            "\n",
            "Test set: Average loss: 1.2587, Accuracy: 51/64 (80%)\n",
            "\n",
            "Train Epoch: 66 [0/64 (0%)]\tLoss: 1.258706\n",
            "\n",
            "Test set: Average loss: 1.2394, Accuracy: 52/64 (81%)\n",
            "\n",
            "Train Epoch: 67 [0/64 (0%)]\tLoss: 1.239412\n",
            "\n",
            "Test set: Average loss: 1.2203, Accuracy: 52/64 (81%)\n",
            "\n",
            "Train Epoch: 68 [0/64 (0%)]\tLoss: 1.220330\n",
            "\n",
            "Test set: Average loss: 1.2014, Accuracy: 52/64 (81%)\n",
            "\n",
            "Train Epoch: 69 [0/64 (0%)]\tLoss: 1.201437\n",
            "\n",
            "Test set: Average loss: 1.1828, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 70 [0/64 (0%)]\tLoss: 1.182771\n",
            "\n",
            "Test set: Average loss: 1.1643, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 71 [0/64 (0%)]\tLoss: 1.164334\n",
            "\n",
            "Test set: Average loss: 1.1461, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 72 [0/64 (0%)]\tLoss: 1.146135\n",
            "\n",
            "Test set: Average loss: 1.1282, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 73 [0/64 (0%)]\tLoss: 1.128189\n",
            "\n",
            "Test set: Average loss: 1.1105, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 74 [0/64 (0%)]\tLoss: 1.110535\n",
            "\n",
            "Test set: Average loss: 1.0932, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 75 [0/64 (0%)]\tLoss: 1.093171\n",
            "\n",
            "Test set: Average loss: 1.0761, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 76 [0/64 (0%)]\tLoss: 1.076087\n",
            "\n",
            "Test set: Average loss: 1.0593, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 77 [0/64 (0%)]\tLoss: 1.059279\n",
            "\n",
            "Test set: Average loss: 1.0427, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 78 [0/64 (0%)]\tLoss: 1.042747\n",
            "\n",
            "Test set: Average loss: 1.0265, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 79 [0/64 (0%)]\tLoss: 1.026503\n",
            "\n",
            "Test set: Average loss: 1.0106, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 80 [0/64 (0%)]\tLoss: 1.010552\n",
            "\n",
            "Test set: Average loss: 0.9949, Accuracy: 53/64 (83%)\n",
            "\n",
            "Train Epoch: 81 [0/64 (0%)]\tLoss: 0.994910\n",
            "\n",
            "Test set: Average loss: 0.9796, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 82 [0/64 (0%)]\tLoss: 0.979581\n",
            "\n",
            "Test set: Average loss: 0.9646, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 83 [0/64 (0%)]\tLoss: 0.964571\n",
            "\n",
            "Test set: Average loss: 0.9499, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 84 [0/64 (0%)]\tLoss: 0.949860\n",
            "\n",
            "Test set: Average loss: 0.9354, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 85 [0/64 (0%)]\tLoss: 0.935435\n",
            "\n",
            "Test set: Average loss: 0.9213, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 86 [0/64 (0%)]\tLoss: 0.921308\n",
            "\n",
            "Test set: Average loss: 0.9075, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 87 [0/64 (0%)]\tLoss: 0.907481\n",
            "\n",
            "Test set: Average loss: 0.8940, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 88 [0/64 (0%)]\tLoss: 0.893953\n",
            "\n",
            "Test set: Average loss: 0.8807, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 89 [0/64 (0%)]\tLoss: 0.880717\n",
            "\n",
            "Test set: Average loss: 0.8678, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 90 [0/64 (0%)]\tLoss: 0.867776\n",
            "\n",
            "Test set: Average loss: 0.8551, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 91 [0/64 (0%)]\tLoss: 0.855112\n",
            "\n",
            "Test set: Average loss: 0.8427, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 92 [0/64 (0%)]\tLoss: 0.842727\n",
            "\n",
            "Test set: Average loss: 0.8306, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 93 [0/64 (0%)]\tLoss: 0.830622\n",
            "\n",
            "Test set: Average loss: 0.8188, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 94 [0/64 (0%)]\tLoss: 0.818776\n",
            "\n",
            "Test set: Average loss: 0.8072, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 95 [0/64 (0%)]\tLoss: 0.807180\n",
            "\n",
            "Test set: Average loss: 0.7958, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 96 [0/64 (0%)]\tLoss: 0.795818\n",
            "\n",
            "Test set: Average loss: 0.7847, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 97 [0/64 (0%)]\tLoss: 0.784697\n",
            "\n",
            "Test set: Average loss: 0.7738, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 98 [0/64 (0%)]\tLoss: 0.773834\n",
            "\n",
            "Test set: Average loss: 0.7632, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 99 [0/64 (0%)]\tLoss: 0.763214\n",
            "\n",
            "Test set: Average loss: 0.7528, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 100 [0/64 (0%)]\tLoss: 0.752825\n",
            "\n",
            "Test set: Average loss: 0.7427, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 101 [0/64 (0%)]\tLoss: 0.742661\n",
            "\n",
            "Test set: Average loss: 0.7327, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 102 [0/64 (0%)]\tLoss: 0.732706\n",
            "\n",
            "Test set: Average loss: 0.7230, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 103 [0/64 (0%)]\tLoss: 0.722951\n",
            "\n",
            "Test set: Average loss: 0.7134, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 104 [0/64 (0%)]\tLoss: 0.713392\n",
            "\n",
            "Test set: Average loss: 0.7040, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 105 [0/64 (0%)]\tLoss: 0.704026\n",
            "\n",
            "Test set: Average loss: 0.6949, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 106 [0/64 (0%)]\tLoss: 0.694876\n",
            "\n",
            "Test set: Average loss: 0.6859, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 107 [0/64 (0%)]\tLoss: 0.685923\n",
            "\n",
            "Test set: Average loss: 0.6771, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 108 [0/64 (0%)]\tLoss: 0.677145\n",
            "\n",
            "Test set: Average loss: 0.6685, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 109 [0/64 (0%)]\tLoss: 0.668535\n",
            "\n",
            "Test set: Average loss: 0.6601, Accuracy: 55/64 (86%)\n",
            "\n",
            "Train Epoch: 110 [0/64 (0%)]\tLoss: 0.660098\n",
            "\n",
            "Test set: Average loss: 0.6518, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 111 [0/64 (0%)]\tLoss: 0.651845\n",
            "\n",
            "Test set: Average loss: 0.6437, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 112 [0/64 (0%)]\tLoss: 0.643748\n",
            "\n",
            "Test set: Average loss: 0.6358, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 113 [0/64 (0%)]\tLoss: 0.635820\n",
            "\n",
            "Test set: Average loss: 0.6281, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 114 [0/64 (0%)]\tLoss: 0.628057\n",
            "\n",
            "Test set: Average loss: 0.6205, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 115 [0/64 (0%)]\tLoss: 0.620465\n",
            "\n",
            "Test set: Average loss: 0.6130, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 116 [0/64 (0%)]\tLoss: 0.613023\n",
            "\n",
            "Test set: Average loss: 0.6057, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 117 [0/64 (0%)]\tLoss: 0.605726\n",
            "\n",
            "Test set: Average loss: 0.5986, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 118 [0/64 (0%)]\tLoss: 0.598582\n",
            "\n",
            "Test set: Average loss: 0.5916, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 119 [0/64 (0%)]\tLoss: 0.591568\n",
            "\n",
            "Test set: Average loss: 0.5847, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 120 [0/64 (0%)]\tLoss: 0.584689\n",
            "\n",
            "Test set: Average loss: 0.5780, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 121 [0/64 (0%)]\tLoss: 0.577969\n",
            "\n",
            "Test set: Average loss: 0.5714, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 122 [0/64 (0%)]\tLoss: 0.571391\n",
            "\n",
            "Test set: Average loss: 0.5649, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 123 [0/64 (0%)]\tLoss: 0.564941\n",
            "\n",
            "Test set: Average loss: 0.5586, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 124 [0/64 (0%)]\tLoss: 0.558608\n",
            "\n",
            "Test set: Average loss: 0.5524, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 125 [0/64 (0%)]\tLoss: 0.552393\n",
            "\n",
            "Test set: Average loss: 0.5463, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 126 [0/64 (0%)]\tLoss: 0.546292\n",
            "\n",
            "Test set: Average loss: 0.5403, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 127 [0/64 (0%)]\tLoss: 0.540324\n",
            "\n",
            "Test set: Average loss: 0.5345, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 128 [0/64 (0%)]\tLoss: 0.534506\n",
            "\n",
            "Test set: Average loss: 0.5288, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 129 [0/64 (0%)]\tLoss: 0.528794\n",
            "\n",
            "Test set: Average loss: 0.5232, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 130 [0/64 (0%)]\tLoss: 0.523185\n",
            "\n",
            "Test set: Average loss: 0.5177, Accuracy: 56/64 (88%)\n",
            "\n",
            "Train Epoch: 131 [0/64 (0%)]\tLoss: 0.517678\n",
            "\n",
            "Test set: Average loss: 0.5123, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 132 [0/64 (0%)]\tLoss: 0.512272\n",
            "\n",
            "Test set: Average loss: 0.5070, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 133 [0/64 (0%)]\tLoss: 0.506970\n",
            "\n",
            "Test set: Average loss: 0.5018, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 134 [0/64 (0%)]\tLoss: 0.501774\n",
            "\n",
            "Test set: Average loss: 0.4967, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 135 [0/64 (0%)]\tLoss: 0.496677\n",
            "\n",
            "Test set: Average loss: 0.4917, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 136 [0/64 (0%)]\tLoss: 0.491669\n",
            "\n",
            "Test set: Average loss: 0.4868, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 137 [0/64 (0%)]\tLoss: 0.486754\n",
            "\n",
            "Test set: Average loss: 0.4819, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 138 [0/64 (0%)]\tLoss: 0.481930\n",
            "\n",
            "Test set: Average loss: 0.4772, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 139 [0/64 (0%)]\tLoss: 0.477195\n",
            "\n",
            "Test set: Average loss: 0.4726, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 140 [0/64 (0%)]\tLoss: 0.472567\n",
            "\n",
            "Test set: Average loss: 0.4680, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 141 [0/64 (0%)]\tLoss: 0.468022\n",
            "\n",
            "Test set: Average loss: 0.4636, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 142 [0/64 (0%)]\tLoss: 0.463557\n",
            "\n",
            "Test set: Average loss: 0.4592, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 143 [0/64 (0%)]\tLoss: 0.459170\n",
            "\n",
            "Test set: Average loss: 0.4549, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 144 [0/64 (0%)]\tLoss: 0.454873\n",
            "\n",
            "Test set: Average loss: 0.4507, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 145 [0/64 (0%)]\tLoss: 0.450658\n",
            "\n",
            "Test set: Average loss: 0.4465, Accuracy: 57/64 (89%)\n",
            "\n",
            "Train Epoch: 146 [0/64 (0%)]\tLoss: 0.446517\n",
            "\n",
            "Test set: Average loss: 0.4425, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 147 [0/64 (0%)]\tLoss: 0.442470\n",
            "\n",
            "Test set: Average loss: 0.4385, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 148 [0/64 (0%)]\tLoss: 0.438498\n",
            "\n",
            "Test set: Average loss: 0.4346, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 149 [0/64 (0%)]\tLoss: 0.434598\n",
            "\n",
            "Test set: Average loss: 0.4308, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 150 [0/64 (0%)]\tLoss: 0.430767\n",
            "\n",
            "Test set: Average loss: 0.4270, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 151 [0/64 (0%)]\tLoss: 0.427004\n",
            "\n",
            "Test set: Average loss: 0.4233, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 152 [0/64 (0%)]\tLoss: 0.423309\n",
            "\n",
            "Test set: Average loss: 0.4197, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 153 [0/64 (0%)]\tLoss: 0.419678\n",
            "\n",
            "Test set: Average loss: 0.4161, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 154 [0/64 (0%)]\tLoss: 0.416111\n",
            "\n",
            "Test set: Average loss: 0.4126, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 155 [0/64 (0%)]\tLoss: 0.412606\n",
            "\n",
            "Test set: Average loss: 0.4092, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 156 [0/64 (0%)]\tLoss: 0.409168\n",
            "\n",
            "Test set: Average loss: 0.4058, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 157 [0/64 (0%)]\tLoss: 0.405800\n",
            "\n",
            "Test set: Average loss: 0.4025, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 158 [0/64 (0%)]\tLoss: 0.402493\n",
            "\n",
            "Test set: Average loss: 0.3992, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 159 [0/64 (0%)]\tLoss: 0.399244\n",
            "\n",
            "Test set: Average loss: 0.3961, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 160 [0/64 (0%)]\tLoss: 0.396054\n",
            "\n",
            "Test set: Average loss: 0.3929, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 161 [0/64 (0%)]\tLoss: 0.392919\n",
            "\n",
            "Test set: Average loss: 0.3898, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 162 [0/64 (0%)]\tLoss: 0.389841\n",
            "\n",
            "Test set: Average loss: 0.3868, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 163 [0/64 (0%)]\tLoss: 0.386819\n",
            "\n",
            "Test set: Average loss: 0.3838, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 164 [0/64 (0%)]\tLoss: 0.383849\n",
            "\n",
            "Test set: Average loss: 0.3809, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 165 [0/64 (0%)]\tLoss: 0.380930\n",
            "\n",
            "Test set: Average loss: 0.3781, Accuracy: 58/64 (91%)\n",
            "\n",
            "Train Epoch: 166 [0/64 (0%)]\tLoss: 0.378060\n",
            "\n",
            "Test set: Average loss: 0.3752, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 167 [0/64 (0%)]\tLoss: 0.375240\n",
            "\n",
            "Test set: Average loss: 0.3725, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 168 [0/64 (0%)]\tLoss: 0.372466\n",
            "\n",
            "Test set: Average loss: 0.3697, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 169 [0/64 (0%)]\tLoss: 0.369738\n",
            "\n",
            "Test set: Average loss: 0.3671, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 170 [0/64 (0%)]\tLoss: 0.367055\n",
            "\n",
            "Test set: Average loss: 0.3644, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 171 [0/64 (0%)]\tLoss: 0.364417\n",
            "\n",
            "Test set: Average loss: 0.3618, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 172 [0/64 (0%)]\tLoss: 0.361824\n",
            "\n",
            "Test set: Average loss: 0.3593, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 173 [0/64 (0%)]\tLoss: 0.359275\n",
            "\n",
            "Test set: Average loss: 0.3568, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 174 [0/64 (0%)]\tLoss: 0.356768\n",
            "\n",
            "Test set: Average loss: 0.3543, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 175 [0/64 (0%)]\tLoss: 0.354304\n",
            "\n",
            "Test set: Average loss: 0.3519, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 176 [0/64 (0%)]\tLoss: 0.351895\n",
            "\n",
            "Test set: Average loss: 0.3496, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 177 [0/64 (0%)]\tLoss: 0.349554\n",
            "\n",
            "Test set: Average loss: 0.3473, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 178 [0/64 (0%)]\tLoss: 0.347260\n",
            "\n",
            "Test set: Average loss: 0.3450, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 179 [0/64 (0%)]\tLoss: 0.345002\n",
            "\n",
            "Test set: Average loss: 0.3428, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 180 [0/64 (0%)]\tLoss: 0.342781\n",
            "\n",
            "Test set: Average loss: 0.3406, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 181 [0/64 (0%)]\tLoss: 0.340596\n",
            "\n",
            "Test set: Average loss: 0.3384, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 182 [0/64 (0%)]\tLoss: 0.338449\n",
            "\n",
            "Test set: Average loss: 0.3363, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 183 [0/64 (0%)]\tLoss: 0.336338\n",
            "\n",
            "Test set: Average loss: 0.3343, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 184 [0/64 (0%)]\tLoss: 0.334263\n",
            "\n",
            "Test set: Average loss: 0.3322, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 185 [0/64 (0%)]\tLoss: 0.332237\n",
            "\n",
            "Test set: Average loss: 0.3302, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 186 [0/64 (0%)]\tLoss: 0.330250\n",
            "\n",
            "Test set: Average loss: 0.3283, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 187 [0/64 (0%)]\tLoss: 0.328294\n",
            "\n",
            "Test set: Average loss: 0.3264, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 188 [0/64 (0%)]\tLoss: 0.326371\n",
            "\n",
            "Test set: Average loss: 0.3245, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 189 [0/64 (0%)]\tLoss: 0.324484\n",
            "\n",
            "Test set: Average loss: 0.3226, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 190 [0/64 (0%)]\tLoss: 0.322627\n",
            "\n",
            "Test set: Average loss: 0.3208, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 191 [0/64 (0%)]\tLoss: 0.320801\n",
            "\n",
            "Test set: Average loss: 0.3190, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 192 [0/64 (0%)]\tLoss: 0.319007\n",
            "\n",
            "Test set: Average loss: 0.3172, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 193 [0/64 (0%)]\tLoss: 0.317247\n",
            "\n",
            "Test set: Average loss: 0.3155, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 194 [0/64 (0%)]\tLoss: 0.315520\n",
            "\n",
            "Test set: Average loss: 0.3138, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 195 [0/64 (0%)]\tLoss: 0.313842\n",
            "\n",
            "Test set: Average loss: 0.3122, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 196 [0/64 (0%)]\tLoss: 0.312192\n",
            "\n",
            "Test set: Average loss: 0.3106, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 197 [0/64 (0%)]\tLoss: 0.310565\n",
            "\n",
            "Test set: Average loss: 0.3090, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 198 [0/64 (0%)]\tLoss: 0.308964\n",
            "\n",
            "Test set: Average loss: 0.3074, Accuracy: 59/64 (92%)\n",
            "\n",
            "Train Epoch: 199 [0/64 (0%)]\tLoss: 0.307388\n",
            "\n",
            "Test set: Average loss: 0.3058, Accuracy: 59/64 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CvFTaNqe3dg",
        "colab_type": "text"
      },
      "source": [
        "We have just applied our basic model on scratch_loader. A decent convnet with good parameters should be able to overfit this data easily.\n",
        "What happened ? What do you conclude ?\n",
        "\n",
        "On viens d'entraîner notre convnet de base sur scratch_loader. Un modèle décent devrait être capable de mémoriser ce dataset aisément. Que s'est-il passé? Qu'en concluez vous?\n",
        "\n",
        "\n",
        "## Build your model\n",
        "\n",
        "It's time to implement your own models to get the best clasification performance on MNIST.\n",
        "First, try to overfit scratch_loader. Once you succeed, replace it by the real loaders and classify these digits!\n",
        "You may consider the following ideas, ranked by relevance:\n",
        "\n",
        "* batch norm\n",
        "* more layers\n",
        "* skip connections\n",
        "* dropout (on the high level features)\n",
        "* data augmentation with `transforms.RandomRotation`or `transforms.RandomAffine` at the dataset creation time\n",
        "\n",
        "If you need some help to understand padding, stride etc..., [here](https://github.com/vdumoulin/conv_arithmetic) is very good resource from a Mila alumni.\n",
        "\n",
        "You can use the cell below to compare the learning curves of your models. Don't forget to change the `'name'`value in the `results`dictionary between each try.\n",
        "\n",
        "## Fabriquez votre modèle\n",
        "\n",
        "Vous devez maintenant implémenter votre propre modèle. Essayez d'abord de mémoriser scratch_loader. Ensuite essayez de bien classifier MNIST. Vous pouvez considérer les idées suivantes classées selon la préférence de l'auteur ce ces lignes:\n",
        "\n",
        "* normalisation de lot \n",
        "* réseau plus profond\n",
        "* connections sautées\n",
        "* abandon de neurones\n",
        "* augmentation de données avec les méthodes `transforms.RandomRotation`ou `transforms.RandomAffine`  lors de la création du jeux de données.\n",
        "\n",
        "Si vous ne connaissez pas ces notions, c'est normal. Regardez la version anglaise. \n",
        "Si vous avez voulez mieux comprendre le fonctionnement des couches convolutionnelles, regardez [ces merveilleux gifs](https://github.com/vdumoulin/conv_arithmetic) réalisés par un ancien du Mila.\n",
        "\n",
        "Vous pouvez utiliser la cellule ci-dessous pour comparer vos courbes d'apprentissages entre elles. N'oubliez pas de changer  la valeur `'name'` dans le dictionnaire `results`  entre chaque essais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etp_txmWe3dg",
        "colab_type": "code",
        "outputId": "69770a43-9fdb-41b7-c04e-b5df75095a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# PLOTTING\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'])\n",
        "            ax1.set_ylabel('cross entropy')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "            ax2.plot(results['accuracy'], label = filename[:-4])\n",
        "            ax2.set_ylabel('accuracy')\n",
        "            ax2.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa969caadd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAEGCAYAAADR+Wn+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMC7QzMzMzArmb5aoeEgoiHn1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHMhlY9re3dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AwesomeNet(nn.Module):\n",
        "    \"\"\"The MNIST killer net.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x[:,:,0,:10]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-2QeRnme3dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HeavyNet(nn.Module):\n",
        "    \"\"\"A medium sized network that performs very well on MNIST.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # conv block 1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # conv block 2\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 32, 14, 14]\n",
        "        \n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 64, 7, 7]\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkfMFV1-e3dq",
        "colab_type": "code",
        "outputId": "f5caa789-5474-4687-b3f3-4de42de5b936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TRAINING\n",
        "model = HeavyNet().to(device)\n",
        "\n",
        "lr = 0.005\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "results = {'name':'awesome', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, valid_loader)\n",
        "    \n",
        "    # save results\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/45056 (0%)]\tLoss: 2.357152\n",
            "Train Epoch: 1 [640/45056 (1%)]\tLoss: 0.346711\n",
            "Train Epoch: 1 [1280/45056 (3%)]\tLoss: 0.348482\n",
            "Train Epoch: 1 [1920/45056 (4%)]\tLoss: 0.116740\n",
            "Train Epoch: 1 [2560/45056 (6%)]\tLoss: 0.173608\n",
            "Train Epoch: 1 [3200/45056 (7%)]\tLoss: 0.176272\n",
            "Train Epoch: 1 [3840/45056 (9%)]\tLoss: 0.316373\n",
            "Train Epoch: 1 [4480/45056 (10%)]\tLoss: 0.060383\n",
            "Train Epoch: 1 [5120/45056 (11%)]\tLoss: 0.075610\n",
            "Train Epoch: 1 [5760/45056 (13%)]\tLoss: 0.064696\n",
            "Train Epoch: 1 [6400/45056 (14%)]\tLoss: 0.125544\n",
            "Train Epoch: 1 [7040/45056 (16%)]\tLoss: 0.133317\n",
            "Train Epoch: 1 [7680/45056 (17%)]\tLoss: 0.021047\n",
            "Train Epoch: 1 [8320/45056 (18%)]\tLoss: 0.103950\n",
            "Train Epoch: 1 [8960/45056 (20%)]\tLoss: 0.034433\n",
            "Train Epoch: 1 [9600/45056 (21%)]\tLoss: 0.145517\n",
            "Train Epoch: 1 [10240/45056 (23%)]\tLoss: 0.265184\n",
            "Train Epoch: 1 [10880/45056 (24%)]\tLoss: 0.030608\n",
            "Train Epoch: 1 [11520/45056 (26%)]\tLoss: 0.104261\n",
            "Train Epoch: 1 [12160/45056 (27%)]\tLoss: 0.158069\n",
            "Train Epoch: 1 [12800/45056 (28%)]\tLoss: 0.171456\n",
            "Train Epoch: 1 [13440/45056 (30%)]\tLoss: 0.024672\n",
            "Train Epoch: 1 [14080/45056 (31%)]\tLoss: 0.033035\n",
            "Train Epoch: 1 [14720/45056 (33%)]\tLoss: 0.294062\n",
            "Train Epoch: 1 [15360/45056 (34%)]\tLoss: 0.258687\n",
            "Train Epoch: 1 [16000/45056 (36%)]\tLoss: 0.283491\n",
            "Train Epoch: 1 [16640/45056 (37%)]\tLoss: 0.015250\n",
            "Train Epoch: 1 [17280/45056 (38%)]\tLoss: 0.055590\n",
            "Train Epoch: 1 [17920/45056 (40%)]\tLoss: 0.054974\n",
            "Train Epoch: 1 [18560/45056 (41%)]\tLoss: 0.071906\n",
            "Train Epoch: 1 [19200/45056 (43%)]\tLoss: 0.229421\n",
            "Train Epoch: 1 [19840/45056 (44%)]\tLoss: 0.042508\n",
            "Train Epoch: 1 [20480/45056 (45%)]\tLoss: 0.121248\n",
            "Train Epoch: 1 [21120/45056 (47%)]\tLoss: 0.015586\n",
            "Train Epoch: 1 [21760/45056 (48%)]\tLoss: 0.078855\n",
            "Train Epoch: 1 [22400/45056 (50%)]\tLoss: 0.066186\n",
            "Train Epoch: 1 [23040/45056 (51%)]\tLoss: 0.095731\n",
            "Train Epoch: 1 [23680/45056 (53%)]\tLoss: 0.130606\n",
            "Train Epoch: 1 [24320/45056 (54%)]\tLoss: 0.080740\n",
            "Train Epoch: 1 [24960/45056 (55%)]\tLoss: 0.042916\n",
            "Train Epoch: 1 [25600/45056 (57%)]\tLoss: 0.053979\n",
            "Train Epoch: 1 [26240/45056 (58%)]\tLoss: 0.064684\n",
            "Train Epoch: 1 [26880/45056 (60%)]\tLoss: 0.013937\n",
            "Train Epoch: 1 [27520/45056 (61%)]\tLoss: 0.014791\n",
            "Train Epoch: 1 [28160/45056 (62%)]\tLoss: 0.154438\n",
            "Train Epoch: 1 [28800/45056 (64%)]\tLoss: 0.117894\n",
            "Train Epoch: 1 [29440/45056 (65%)]\tLoss: 0.099784\n",
            "Train Epoch: 1 [30080/45056 (67%)]\tLoss: 0.157827\n",
            "Train Epoch: 1 [30720/45056 (68%)]\tLoss: 0.024462\n",
            "Train Epoch: 1 [31360/45056 (70%)]\tLoss: 0.083595\n",
            "Train Epoch: 1 [32000/45056 (71%)]\tLoss: 0.092910\n",
            "Train Epoch: 1 [32640/45056 (72%)]\tLoss: 0.124244\n",
            "Train Epoch: 1 [33280/45056 (74%)]\tLoss: 0.059593\n",
            "Train Epoch: 1 [33920/45056 (75%)]\tLoss: 0.269946\n",
            "Train Epoch: 1 [34560/45056 (77%)]\tLoss: 0.048918\n",
            "Train Epoch: 1 [35200/45056 (78%)]\tLoss: 0.110269\n",
            "Train Epoch: 1 [35840/45056 (80%)]\tLoss: 0.031236\n",
            "Train Epoch: 1 [36480/45056 (81%)]\tLoss: 0.007661\n",
            "Train Epoch: 1 [37120/45056 (82%)]\tLoss: 0.031976\n",
            "Train Epoch: 1 [37760/45056 (84%)]\tLoss: 0.063418\n",
            "Train Epoch: 1 [38400/45056 (85%)]\tLoss: 0.003818\n",
            "Train Epoch: 1 [39040/45056 (87%)]\tLoss: 0.140200\n",
            "Train Epoch: 1 [39680/45056 (88%)]\tLoss: 0.180837\n",
            "Train Epoch: 1 [40320/45056 (89%)]\tLoss: 0.061540\n",
            "Train Epoch: 1 [40960/45056 (91%)]\tLoss: 0.158979\n",
            "Train Epoch: 1 [41600/45056 (92%)]\tLoss: 0.017433\n",
            "Train Epoch: 1 [42240/45056 (94%)]\tLoss: 0.046619\n",
            "Train Epoch: 1 [42880/45056 (95%)]\tLoss: 0.103361\n",
            "Train Epoch: 1 [43520/45056 (97%)]\tLoss: 0.062257\n",
            "Train Epoch: 1 [44160/45056 (98%)]\tLoss: 0.052482\n",
            "Train Epoch: 1 [44800/45056 (99%)]\tLoss: 0.094849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fa9b014f550>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0560, Accuracy: 14720/15000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/45056 (0%)]\tLoss: 0.287840\n",
            "Train Epoch: 2 [640/45056 (1%)]\tLoss: 0.049669\n",
            "Train Epoch: 2 [1280/45056 (3%)]\tLoss: 0.093962\n",
            "Train Epoch: 2 [1920/45056 (4%)]\tLoss: 0.009742\n",
            "Train Epoch: 2 [2560/45056 (6%)]\tLoss: 0.114549\n",
            "Train Epoch: 2 [3200/45056 (7%)]\tLoss: 0.009344\n",
            "Train Epoch: 2 [3840/45056 (9%)]\tLoss: 0.016615\n",
            "Train Epoch: 2 [4480/45056 (10%)]\tLoss: 0.070963\n",
            "Train Epoch: 2 [5120/45056 (11%)]\tLoss: 0.003091\n",
            "Train Epoch: 2 [5760/45056 (13%)]\tLoss: 0.009339\n",
            "Train Epoch: 2 [6400/45056 (14%)]\tLoss: 0.145449\n",
            "Train Epoch: 2 [7040/45056 (16%)]\tLoss: 0.183803\n",
            "Train Epoch: 2 [7680/45056 (17%)]\tLoss: 0.015950\n",
            "Train Epoch: 2 [8320/45056 (18%)]\tLoss: 0.047707\n",
            "Train Epoch: 2 [8960/45056 (20%)]\tLoss: 0.001308\n",
            "Train Epoch: 2 [9600/45056 (21%)]\tLoss: 0.026700\n",
            "Train Epoch: 2 [10240/45056 (23%)]\tLoss: 0.000821\n",
            "Train Epoch: 2 [10880/45056 (24%)]\tLoss: 0.081010\n",
            "Train Epoch: 2 [11520/45056 (26%)]\tLoss: 0.023017\n",
            "Train Epoch: 2 [12160/45056 (27%)]\tLoss: 0.075832\n",
            "Train Epoch: 2 [12800/45056 (28%)]\tLoss: 0.017749\n",
            "Train Epoch: 2 [13440/45056 (30%)]\tLoss: 0.051824\n",
            "Train Epoch: 2 [14080/45056 (31%)]\tLoss: 0.022729\n",
            "Train Epoch: 2 [14720/45056 (33%)]\tLoss: 0.070623\n",
            "Train Epoch: 2 [15360/45056 (34%)]\tLoss: 0.136385\n",
            "Train Epoch: 2 [16000/45056 (36%)]\tLoss: 0.018000\n",
            "Train Epoch: 2 [16640/45056 (37%)]\tLoss: 0.027454\n",
            "Train Epoch: 2 [17280/45056 (38%)]\tLoss: 0.019211\n",
            "Train Epoch: 2 [17920/45056 (40%)]\tLoss: 0.138183\n",
            "Train Epoch: 2 [18560/45056 (41%)]\tLoss: 0.027188\n",
            "Train Epoch: 2 [19200/45056 (43%)]\tLoss: 0.011080\n",
            "Train Epoch: 2 [19840/45056 (44%)]\tLoss: 0.112246\n",
            "Train Epoch: 2 [20480/45056 (45%)]\tLoss: 0.059767\n",
            "Train Epoch: 2 [21120/45056 (47%)]\tLoss: 0.063407\n",
            "Train Epoch: 2 [21760/45056 (48%)]\tLoss: 0.003329\n",
            "Train Epoch: 2 [22400/45056 (50%)]\tLoss: 0.075844\n",
            "Train Epoch: 2 [23040/45056 (51%)]\tLoss: 0.001312\n",
            "Train Epoch: 2 [23680/45056 (53%)]\tLoss: 0.002397\n",
            "Train Epoch: 2 [24320/45056 (54%)]\tLoss: 0.022777\n",
            "Train Epoch: 2 [24960/45056 (55%)]\tLoss: 0.027450\n",
            "Train Epoch: 2 [25600/45056 (57%)]\tLoss: 0.098649\n",
            "Train Epoch: 2 [26240/45056 (58%)]\tLoss: 0.010208\n",
            "Train Epoch: 2 [26880/45056 (60%)]\tLoss: 0.074750\n",
            "Train Epoch: 2 [27520/45056 (61%)]\tLoss: 0.034783\n",
            "Train Epoch: 2 [28160/45056 (62%)]\tLoss: 0.100613\n",
            "Train Epoch: 2 [28800/45056 (64%)]\tLoss: 0.052139\n",
            "Train Epoch: 2 [29440/45056 (65%)]\tLoss: 0.100246\n",
            "Train Epoch: 2 [30080/45056 (67%)]\tLoss: 0.016882\n",
            "Train Epoch: 2 [30720/45056 (68%)]\tLoss: 0.005291\n",
            "Train Epoch: 2 [31360/45056 (70%)]\tLoss: 0.006135\n",
            "Train Epoch: 2 [32000/45056 (71%)]\tLoss: 0.192427\n",
            "Train Epoch: 2 [32640/45056 (72%)]\tLoss: 0.066120\n",
            "Train Epoch: 2 [33280/45056 (74%)]\tLoss: 0.016830\n",
            "Train Epoch: 2 [33920/45056 (75%)]\tLoss: 0.071245\n",
            "Train Epoch: 2 [34560/45056 (77%)]\tLoss: 0.010761\n",
            "Train Epoch: 2 [35200/45056 (78%)]\tLoss: 0.016851\n",
            "Train Epoch: 2 [35840/45056 (80%)]\tLoss: 0.036402\n",
            "Train Epoch: 2 [36480/45056 (81%)]\tLoss: 0.010197\n",
            "Train Epoch: 2 [37120/45056 (82%)]\tLoss: 0.020893\n",
            "Train Epoch: 2 [37760/45056 (84%)]\tLoss: 0.022890\n",
            "Train Epoch: 2 [38400/45056 (85%)]\tLoss: 0.031610\n",
            "Train Epoch: 2 [39040/45056 (87%)]\tLoss: 0.005844\n",
            "Train Epoch: 2 [39680/45056 (88%)]\tLoss: 0.009213\n",
            "Train Epoch: 2 [40320/45056 (89%)]\tLoss: 0.031011\n",
            "Train Epoch: 2 [40960/45056 (91%)]\tLoss: 0.111373\n",
            "Train Epoch: 2 [41600/45056 (92%)]\tLoss: 0.004494\n",
            "Train Epoch: 2 [42240/45056 (94%)]\tLoss: 0.011398\n",
            "Train Epoch: 2 [42880/45056 (95%)]\tLoss: 0.077670\n",
            "Train Epoch: 2 [43520/45056 (97%)]\tLoss: 0.007199\n",
            "Train Epoch: 2 [44160/45056 (98%)]\tLoss: 0.245826\n",
            "Train Epoch: 2 [44800/45056 (99%)]\tLoss: 0.014537\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 14807/15000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/45056 (0%)]\tLoss: 0.000530\n",
            "Train Epoch: 3 [640/45056 (1%)]\tLoss: 0.026667\n",
            "Train Epoch: 3 [1280/45056 (3%)]\tLoss: 0.017848\n",
            "Train Epoch: 3 [1920/45056 (4%)]\tLoss: 0.022484\n",
            "Train Epoch: 3 [2560/45056 (6%)]\tLoss: 0.008660\n",
            "Train Epoch: 3 [3200/45056 (7%)]\tLoss: 0.127873\n",
            "Train Epoch: 3 [3840/45056 (9%)]\tLoss: 0.022675\n",
            "Train Epoch: 3 [4480/45056 (10%)]\tLoss: 0.041245\n",
            "Train Epoch: 3 [5120/45056 (11%)]\tLoss: 0.021491\n",
            "Train Epoch: 3 [5760/45056 (13%)]\tLoss: 0.021697\n",
            "Train Epoch: 3 [6400/45056 (14%)]\tLoss: 0.012126\n",
            "Train Epoch: 3 [7040/45056 (16%)]\tLoss: 0.034236\n",
            "Train Epoch: 3 [7680/45056 (17%)]\tLoss: 0.041657\n",
            "Train Epoch: 3 [8320/45056 (18%)]\tLoss: 0.004843\n",
            "Train Epoch: 3 [8960/45056 (20%)]\tLoss: 0.008936\n",
            "Train Epoch: 3 [9600/45056 (21%)]\tLoss: 0.013713\n",
            "Train Epoch: 3 [10240/45056 (23%)]\tLoss: 0.012662\n",
            "Train Epoch: 3 [10880/45056 (24%)]\tLoss: 0.051042\n",
            "Train Epoch: 3 [11520/45056 (26%)]\tLoss: 0.012311\n",
            "Train Epoch: 3 [12160/45056 (27%)]\tLoss: 0.016814\n",
            "Train Epoch: 3 [12800/45056 (28%)]\tLoss: 0.024419\n",
            "Train Epoch: 3 [13440/45056 (30%)]\tLoss: 0.008855\n",
            "Train Epoch: 3 [14080/45056 (31%)]\tLoss: 0.094417\n",
            "Train Epoch: 3 [14720/45056 (33%)]\tLoss: 0.004447\n",
            "Train Epoch: 3 [15360/45056 (34%)]\tLoss: 0.079257\n",
            "Train Epoch: 3 [16000/45056 (36%)]\tLoss: 0.040534\n",
            "Train Epoch: 3 [16640/45056 (37%)]\tLoss: 0.036216\n",
            "Train Epoch: 3 [17280/45056 (38%)]\tLoss: 0.020127\n",
            "Train Epoch: 3 [17920/45056 (40%)]\tLoss: 0.004940\n",
            "Train Epoch: 3 [18560/45056 (41%)]\tLoss: 0.004542\n",
            "Train Epoch: 3 [19200/45056 (43%)]\tLoss: 0.114859\n",
            "Train Epoch: 3 [19840/45056 (44%)]\tLoss: 0.078222\n",
            "Train Epoch: 3 [20480/45056 (45%)]\tLoss: 0.017088\n",
            "Train Epoch: 3 [21120/45056 (47%)]\tLoss: 0.045462\n",
            "Train Epoch: 3 [21760/45056 (48%)]\tLoss: 0.024784\n",
            "Train Epoch: 3 [22400/45056 (50%)]\tLoss: 0.038641\n",
            "Train Epoch: 3 [23040/45056 (51%)]\tLoss: 0.009826\n",
            "Train Epoch: 3 [23680/45056 (53%)]\tLoss: 0.007499\n",
            "Train Epoch: 3 [24320/45056 (54%)]\tLoss: 0.235768\n",
            "Train Epoch: 3 [24960/45056 (55%)]\tLoss: 0.002007\n",
            "Train Epoch: 3 [25600/45056 (57%)]\tLoss: 0.035872\n",
            "Train Epoch: 3 [26240/45056 (58%)]\tLoss: 0.006825\n",
            "Train Epoch: 3 [26880/45056 (60%)]\tLoss: 0.001448\n",
            "Train Epoch: 3 [27520/45056 (61%)]\tLoss: 0.010726\n",
            "Train Epoch: 3 [28160/45056 (62%)]\tLoss: 0.049269\n",
            "Train Epoch: 3 [28800/45056 (64%)]\tLoss: 0.056411\n",
            "Train Epoch: 3 [29440/45056 (65%)]\tLoss: 0.134543\n",
            "Train Epoch: 3 [30080/45056 (67%)]\tLoss: 0.057928\n",
            "Train Epoch: 3 [30720/45056 (68%)]\tLoss: 0.015135\n",
            "Train Epoch: 3 [31360/45056 (70%)]\tLoss: 0.007943\n",
            "Train Epoch: 3 [32000/45056 (71%)]\tLoss: 0.046864\n",
            "Train Epoch: 3 [32640/45056 (72%)]\tLoss: 0.007550\n",
            "Train Epoch: 3 [33280/45056 (74%)]\tLoss: 0.012981\n",
            "Train Epoch: 3 [33920/45056 (75%)]\tLoss: 0.009102\n",
            "Train Epoch: 3 [34560/45056 (77%)]\tLoss: 0.079702\n",
            "Train Epoch: 3 [35200/45056 (78%)]\tLoss: 0.044781\n",
            "Train Epoch: 3 [35840/45056 (80%)]\tLoss: 0.162169\n",
            "Train Epoch: 3 [36480/45056 (81%)]\tLoss: 0.026368\n",
            "Train Epoch: 3 [37120/45056 (82%)]\tLoss: 0.011707\n",
            "Train Epoch: 3 [37760/45056 (84%)]\tLoss: 0.012189\n",
            "Train Epoch: 3 [38400/45056 (85%)]\tLoss: 0.066343\n",
            "Train Epoch: 3 [39040/45056 (87%)]\tLoss: 0.010715\n",
            "Train Epoch: 3 [39680/45056 (88%)]\tLoss: 0.122811\n",
            "Train Epoch: 3 [40320/45056 (89%)]\tLoss: 0.036357\n",
            "Train Epoch: 3 [40960/45056 (91%)]\tLoss: 0.042053\n",
            "Train Epoch: 3 [41600/45056 (92%)]\tLoss: 0.003359\n",
            "Train Epoch: 3 [42240/45056 (94%)]\tLoss: 0.008883\n",
            "Train Epoch: 3 [42880/45056 (95%)]\tLoss: 0.019960\n",
            "Train Epoch: 3 [43520/45056 (97%)]\tLoss: 0.001305\n",
            "Train Epoch: 3 [44160/45056 (98%)]\tLoss: 0.035547\n",
            "Train Epoch: 3 [44800/45056 (99%)]\tLoss: 0.058691\n",
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 14801/15000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/45056 (0%)]\tLoss: 0.002266\n",
            "Train Epoch: 4 [640/45056 (1%)]\tLoss: 0.017056\n",
            "Train Epoch: 4 [1280/45056 (3%)]\tLoss: 0.081834\n",
            "Train Epoch: 4 [1920/45056 (4%)]\tLoss: 0.077596\n",
            "Train Epoch: 4 [2560/45056 (6%)]\tLoss: 0.003035\n",
            "Train Epoch: 4 [3200/45056 (7%)]\tLoss: 0.045053\n",
            "Train Epoch: 4 [3840/45056 (9%)]\tLoss: 0.087643\n",
            "Train Epoch: 4 [4480/45056 (10%)]\tLoss: 0.000807\n",
            "Train Epoch: 4 [5120/45056 (11%)]\tLoss: 0.004557\n",
            "Train Epoch: 4 [5760/45056 (13%)]\tLoss: 0.006746\n",
            "Train Epoch: 4 [6400/45056 (14%)]\tLoss: 0.021383\n",
            "Train Epoch: 4 [7040/45056 (16%)]\tLoss: 0.005232\n",
            "Train Epoch: 4 [7680/45056 (17%)]\tLoss: 0.025883\n",
            "Train Epoch: 4 [8320/45056 (18%)]\tLoss: 0.029440\n",
            "Train Epoch: 4 [8960/45056 (20%)]\tLoss: 0.001055\n",
            "Train Epoch: 4 [9600/45056 (21%)]\tLoss: 0.102802\n",
            "Train Epoch: 4 [10240/45056 (23%)]\tLoss: 0.025112\n",
            "Train Epoch: 4 [10880/45056 (24%)]\tLoss: 0.034152\n",
            "Train Epoch: 4 [11520/45056 (26%)]\tLoss: 0.002928\n",
            "Train Epoch: 4 [12160/45056 (27%)]\tLoss: 0.030174\n",
            "Train Epoch: 4 [12800/45056 (28%)]\tLoss: 0.001304\n",
            "Train Epoch: 4 [13440/45056 (30%)]\tLoss: 0.001869\n",
            "Train Epoch: 4 [14080/45056 (31%)]\tLoss: 0.006032\n",
            "Train Epoch: 4 [14720/45056 (33%)]\tLoss: 0.017086\n",
            "Train Epoch: 4 [15360/45056 (34%)]\tLoss: 0.019958\n",
            "Train Epoch: 4 [16000/45056 (36%)]\tLoss: 0.050666\n",
            "Train Epoch: 4 [16640/45056 (37%)]\tLoss: 0.001995\n",
            "Train Epoch: 4 [17280/45056 (38%)]\tLoss: 0.020765\n",
            "Train Epoch: 4 [17920/45056 (40%)]\tLoss: 0.005771\n",
            "Train Epoch: 4 [18560/45056 (41%)]\tLoss: 0.016587\n",
            "Train Epoch: 4 [19200/45056 (43%)]\tLoss: 0.006774\n",
            "Train Epoch: 4 [19840/45056 (44%)]\tLoss: 0.025707\n",
            "Train Epoch: 4 [20480/45056 (45%)]\tLoss: 0.008838\n",
            "Train Epoch: 4 [21120/45056 (47%)]\tLoss: 0.036246\n",
            "Train Epoch: 4 [21760/45056 (48%)]\tLoss: 0.038185\n",
            "Train Epoch: 4 [22400/45056 (50%)]\tLoss: 0.013966\n",
            "Train Epoch: 4 [23040/45056 (51%)]\tLoss: 0.054804\n",
            "Train Epoch: 4 [23680/45056 (53%)]\tLoss: 0.001941\n",
            "Train Epoch: 4 [24320/45056 (54%)]\tLoss: 0.045119\n",
            "Train Epoch: 4 [24960/45056 (55%)]\tLoss: 0.093333\n",
            "Train Epoch: 4 [25600/45056 (57%)]\tLoss: 0.010749\n",
            "Train Epoch: 4 [26240/45056 (58%)]\tLoss: 0.044103\n",
            "Train Epoch: 4 [26880/45056 (60%)]\tLoss: 0.061806\n",
            "Train Epoch: 4 [27520/45056 (61%)]\tLoss: 0.072298\n",
            "Train Epoch: 4 [28160/45056 (62%)]\tLoss: 0.373770\n",
            "Train Epoch: 4 [28800/45056 (64%)]\tLoss: 0.003592\n",
            "Train Epoch: 4 [29440/45056 (65%)]\tLoss: 0.037975\n",
            "Train Epoch: 4 [30080/45056 (67%)]\tLoss: 0.101832\n",
            "Train Epoch: 4 [30720/45056 (68%)]\tLoss: 0.001114\n",
            "Train Epoch: 4 [31360/45056 (70%)]\tLoss: 0.007977\n",
            "Train Epoch: 4 [32000/45056 (71%)]\tLoss: 0.024316\n",
            "Train Epoch: 4 [32640/45056 (72%)]\tLoss: 0.005553\n",
            "Train Epoch: 4 [33280/45056 (74%)]\tLoss: 0.055295\n",
            "Train Epoch: 4 [33920/45056 (75%)]\tLoss: 0.029925\n",
            "Train Epoch: 4 [34560/45056 (77%)]\tLoss: 0.021857\n",
            "Train Epoch: 4 [35200/45056 (78%)]\tLoss: 0.044950\n",
            "Train Epoch: 4 [35840/45056 (80%)]\tLoss: 0.001310\n",
            "Train Epoch: 4 [36480/45056 (81%)]\tLoss: 0.064506\n",
            "Train Epoch: 4 [37120/45056 (82%)]\tLoss: 0.019806\n",
            "Train Epoch: 4 [37760/45056 (84%)]\tLoss: 0.006496\n",
            "Train Epoch: 4 [38400/45056 (85%)]\tLoss: 0.000686\n",
            "Train Epoch: 4 [39040/45056 (87%)]\tLoss: 0.001559\n",
            "Train Epoch: 4 [39680/45056 (88%)]\tLoss: 0.018485\n",
            "Train Epoch: 4 [40320/45056 (89%)]\tLoss: 0.017097\n",
            "Train Epoch: 4 [40960/45056 (91%)]\tLoss: 0.004768\n",
            "Train Epoch: 4 [41600/45056 (92%)]\tLoss: 0.000284\n",
            "Train Epoch: 4 [42240/45056 (94%)]\tLoss: 0.013200\n",
            "Train Epoch: 4 [42880/45056 (95%)]\tLoss: 0.004405\n",
            "Train Epoch: 4 [43520/45056 (97%)]\tLoss: 0.001423\n",
            "Train Epoch: 4 [44160/45056 (98%)]\tLoss: 0.004569\n",
            "Train Epoch: 4 [44800/45056 (99%)]\tLoss: 0.000987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d55f4d74f806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3538ed793d07>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRtZ1SnTe3dz",
        "colab_type": "text"
      },
      "source": [
        "If you want to learn more about Pytorch, here is a very comprehensive [tutorial](https://nbviewer.jupyter.org/github/ds4dm/tipsntricks/blob/master/pytorch/tutorial.ipynb) made by Mila for Mila. You are encouraged to look at it **after** this lab session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc6WOP4We3d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}