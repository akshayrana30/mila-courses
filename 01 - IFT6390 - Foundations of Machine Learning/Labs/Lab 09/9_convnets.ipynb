{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_convnets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvGeGKRUe3cv",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks (CNN)\n",
        "\n",
        "Welcome to the wonderworld of CNNs™. If you followed the last lab session on pytorch, you should already be comfortable with Pytorch's autograd mechanism, and with the standard way to train a model. Today we will focus on convolutional neural nets, aka CNN, aka convnets. On the way, we will also get more familiar with the data loading mechanism from pytorch and the training loop. You have two goals:\n",
        "\n",
        "1. Overfit a small subset of MNIST. This is a fast and reliable way to test that a model is not garbage.\n",
        "2. reach the best possible test score on MNIST. Bon courage.\n",
        "\n",
        "# Réseaux de Neurones Convolutionnels (RNC)\n",
        "###### C'est une blague, personne n'utilise cet acronyme.\n",
        "\n",
        "Bienvenu dans le monde merveilleux des CNNs™. Si vous avez suivi la dernière démonstration sur pytorch, vous devriez déjà être à l'aise avec le mécanisme de différentiation automatique, ainsi qu'avec la méthode standard pour entraîner un modèle. Aujourd'hui,  on va s'intéresser à toutes les subtilités des réseaux de neurones convolutionnels. Au passage on va revoir le mécanisme de chargement des données et la boucle d'entraînement. Vous avez deux buts:\n",
        "\n",
        "1. mémoriser une petite fraction de MNIST avec un modèle. C'est une méthode simple et efficace pour vérifier qu'un modèle est décent.\n",
        "2. atteindre le meilleur score possible sur MNIST entier. Good luck.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhfFTl1pe3cw",
        "colab_type": "text"
      },
      "source": [
        "## Set up / Préparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20cHv52zfXT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ07Jtthe3cy",
        "colab_type": "code",
        "outputId": "2bfb933c-a963-41f7-d1e1-d8ec6faa5772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(f\"Your version of Pytorch is {torch.__version__}.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your version of Pytorch is 1.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JizO0m1Ze3c6",
        "colab_type": "code",
        "outputId": "88c0a4cb-7244-4089-f85a-2dfa3d59106e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# If a GPU is available, use it\n",
        "# Pytorch uses an elegant way to keep the code device agnostic\n",
        "# Si un GPU est disponible, l'utilise\n",
        "# PyTorch a une façon élegante de garder le code indépendant du device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoIKi7ebe3c9",
        "colab_type": "text"
      },
      "source": [
        "## Datasets / Bases de Données\n",
        "\n",
        "We can use `torchvision`, the vision processing and data library in`PyTorch`, to load `MNIST` more easily. We convert it to a `Tensor` and also normalize the values so our model has an easier time fitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK9fKqhoe3c9",
        "colab_type": "code",
        "outputId": "2375b029-cbad-4cd4-f4e7-22d7d5fe1c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       # Standardize with mean and std computed on train set\n",
        "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                   ]))\n",
        "\n",
        "# TODO: define the test dataset\n",
        "# A FAIRE : construire le jeu de données de test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8519886.49it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 129322.66it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2004330.89it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49537.62it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lknHPBxSe3dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of the scratch training set TO OVERFIT\n",
        "# taille du jeu d'entrainement d'essai, que l'on souhaite overfit\n",
        "n_scratch = 64\n",
        "\n",
        "# This parameter influences optimization\n",
        "# Ce paramètre influence l'optimisation\n",
        "batch_size = 32\n",
        "# This is just for evaluation, we want is as big as the GPU can support\n",
        "# Ceci est juste pour l'évaluation, on le veut aussi grand que le GPU le permet\n",
        "batch_size_eval = 512\n",
        "\n",
        "\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwMOe_VkvneF",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading / Chargement de données\n",
        "\n",
        "While `Dataset` holds the data, we use `DataLoader` to choose how we extract the data and turn it into inputs we feed our model.\n",
        "\n",
        "Our `sampler` class decides how we take the data from the `Dataset`, from shuffling to subsets, see [the docs](https://pytorch.org/docs/stable/data.html) for more info\n",
        "\n",
        "/\n",
        "\n",
        "Alors que `Dataset` contient les données, nous utilisons `DataLoader` pour choisir comme extraire les données et les transformer en entrées pour notre modèle.\n",
        "\n",
        "Notre classe `sampler` decide comment nous prenons les données du `Dataset`, par exemple en mélangeant ou en choisissant au sein d'un sous-ensemble. Pour plus d'informations, voir [les docs](https://pytorch.org/docs/stable/data.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3AIrdce3dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import (SubsetRandomSampler,\n",
        "                                      RandomSampler)\n",
        "\n",
        "# This is the subset of MNIST we want to overfit\n",
        "# C'est le sous-ensemble de MNIST que nous voulons overfit\n",
        "scratch_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    # The sampler is an easy way to say that we're using the elements\n",
        "    # `indices[:n_scratch]` for this loader\n",
        "    # Le sampler est une façon simple de dire que nous utilisons les\n",
        "    # elements `indices[:n_scratch]` pour ce loader\n",
        "    sampler=SubsetRandomSampler(indices[:n_scratch]),\n",
        "    num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI94_rCawyTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: We now want to define a data loader for the training set, validation set and test \n",
        "# set of the whole MNIST. The test set has already been loaded earlier, and for the validation\n",
        "# set, we will use n_valid examples taken from the training data (they must be sampled only by the\n",
        "# validation data loader, not the training data loader)\n",
        "# define your own data loaders for the training, validation and test datasets\n",
        "\n",
        "# A FAIRE: Nous voulons maintenant definir un chargeur de données pour l'ensemble d'entrainement,\n",
        "# l'ensemble de validation, et l'ensemble de test de MNIST entier. L'ensemble de test a déjà été chargé plus\n",
        "# tôt, et pour l'ensemble de validation, nous allons utiliser n_valid exemples pris des données d'entrainement\n",
        "# (elles doivent être utilisées uniquement par le chargeur de données de validation, pas celui d'entrainement)\n",
        "# Definir vos propres chargeurs de données pour l'entrainement, la validation et le test.\n",
        "\n",
        "n_valid = 15000\n",
        "\n",
        "indices[:n_valid] # valid\n",
        "indices[n_valid:] # train\n",
        "\n",
        "train_loader = DataLoader()\n",
        "valid_loader = DataLoader()\n",
        "test_loader = Dataloader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFJcFGi6e3dG",
        "colab_type": "code",
        "outputId": "bc42262e-bc39-4087-9271-96af3c0ed40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# visualize and understand the data\n",
        "# TODO: print shapes of first batch and target and visualize the first digit.\n",
        "# What is the meaning of each dimension of the batch ?\n",
        "\n",
        "# Visualiser et comprendre les données\n",
        "# A FAIRE: Afficher la forme des premiers batch et objectifs et visualiser le premier chiffre.\n",
        "# À quoi correspondent chaque dimension du batch ?\n",
        "\n",
        "for inputs, targets in scratch_loader:\n",
        "    \n",
        "    # print shape of input batchs and targets\n",
        "    # afficher la forme des batch d'entrées et des objectifs\n",
        "\n",
        "    img = [[0]]# TODO / A FAIRE\n",
        "    plt.imshow(img, cmap='Greys_r')\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD4CAYAAADhGCPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUss4SGoZB0kt4yCpZRwktf4XfyoRHTASQm4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4lapoWe3dL",
        "colab_type": "text"
      },
      "source": [
        "## Models / Modèles\n",
        "You can find below a basic CNN. You will have to define your own model a bit later. First let's try to train this one!\n",
        "\n",
        "Vous avez ci-dessous un CNN élémentaire. Vous devrez définir votre propre modèle un peu plus tard. Pour l'instant essayons déjà d'entraîner celui-ci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30OitDs4e3dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicNet(nn.Module):\n",
        "    \"\"\"Affordable convolutions for the people.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        self.fc = nn.Linear(64*7*7, 10)\n",
        "\n",
        "    def forward(self, xin):\n",
        "        # xin is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        \n",
        "        x = F.relu(self.conv1(xin))\n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # x is [bs, 32, 14, 14] \n",
        "        \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # x is [bs, 64, 7, 7]\n",
        "        \n",
        "        #TODO : We now flatten x. What shape should we give it ?\n",
        "        #A FAIRE : Nous voulons maintenant applatir x. Quelle forme devrions nous lui donner ?\n",
        "        flatten_size = 1 * something * something\n",
        "        x = x.view(-1, flatten_size)\n",
        "        x = F.relu(self.fc(x))\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrCD4Mjfe3dO",
        "colab_type": "text"
      },
      "source": [
        "## Training / Entraînement\n",
        "\n",
        "You have to define general training and testing loops that can be applied to any pytorch module.\n",
        "\n",
        "Vous devez définir des boucles d'entraînement et de test générales qui s'appliquent à n'importe quel module pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agd2qklAe3dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Surrogate loss used for training\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# spot to save your learning curves, and potentially checkpoint your models\n",
        "savedir = 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d6q1p3Ee3dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,train_loader, optimizer, epoch ):\n",
        "    \"\"\"Perform one epoch of training.\"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "\n",
        "        # TODO: code this training loop. Remember lab 8 (or take a quick look)\n",
        "        # A FAIRE: Implementer cette boucle d'entrainement. Rappelez vous du lab 8 (ou jettez y un coup d'oeil)\n",
        "        loss = torch.zeros(1)\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls48bZZGe3dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0 # total loss over test set\n",
        "    correct = 0 # total number of correct test prediction\n",
        "    test_size = 0 # number of test samples used\n",
        "    \n",
        "    with torch.no_grad(): # save some computations\n",
        "      \n",
        "        for inputs, target in test_loader:\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "\n",
        "            # TODO: code the evaluation loop\n",
        "            # A FAIRE: implementez la boucle d'evaluation\n",
        "            test_size += len(inputs)\n",
        "            #\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_size, 100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTy5iKHe3dY",
        "colab_type": "code",
        "outputId": "93167791-3a4e-449a-b84a-f63f629e5225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BasicNet().to(device)\n",
        "\n",
        "lr = 0.0005\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train(model, scratch_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, scratch_loader)\n",
        "    \n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 2 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 3 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 4 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 5 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 6 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 7 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 8 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 9 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 10 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 11 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 12 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 13 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 14 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 15 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 16 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 17 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 18 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 19 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 20 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 21 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 22 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 23 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 24 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 25 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 26 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 27 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 28 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 29 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 30 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 31 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 32 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 33 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 34 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 35 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 36 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 37 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 38 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 39 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 40 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 41 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 42 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 43 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 44 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 45 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 46 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 47 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 48 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 49 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 50 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 51 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 52 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 53 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 54 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 55 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 56 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 57 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 58 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 59 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 60 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 61 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 62 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 63 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 64 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 65 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 66 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 67 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 68 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 69 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 70 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 71 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 72 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 73 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 74 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 75 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 76 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 77 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 78 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 79 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 80 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 81 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 82 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 83 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 84 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 85 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 86 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 87 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 88 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 89 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 90 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 91 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 92 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 93 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 94 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 95 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 96 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 97 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 98 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 99 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 100 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 101 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 102 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 103 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 104 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 105 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 106 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 107 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 108 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 109 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 110 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 111 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 112 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 113 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 114 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 115 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 116 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 117 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 118 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 119 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 120 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 121 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 122 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 123 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 124 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 125 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 126 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 127 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 128 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 129 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 130 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 131 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 132 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 133 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 134 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 135 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 136 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 137 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 138 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 139 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 140 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 141 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 142 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 143 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 144 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 145 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 146 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 147 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 148 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 149 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 150 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 151 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 152 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 153 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 154 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 155 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 156 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 157 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 158 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 159 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 160 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 161 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 162 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 163 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 164 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 165 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 166 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 167 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 168 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 169 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 170 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 171 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 172 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 173 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 174 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 175 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 176 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 177 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 178 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 179 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 180 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 181 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 182 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 183 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 184 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 185 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 186 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 187 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 188 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 189 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 190 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 191 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 192 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 193 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 194 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 195 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 196 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 197 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 198 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n",
            "Train Epoch: 199 [0/64 (0%)]\tLoss: 0.000000\n",
            "Test set: Average loss: 0.0000, Accuracy: 0/64 (0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CvFTaNqe3dg",
        "colab_type": "text"
      },
      "source": [
        "We have just applied our basic model on scratch_loader. A decent convnet with good parameters should be able to overfit this data easily.\n",
        "What happened ? What do you conclude ?\n",
        "\n",
        "On viens d'entraîner notre convnet de base sur scratch_loader. Un modèle décent devrait être capable de mémoriser ce dataset aisément. Que s'est-il passé? Qu'en concluez vous?\n",
        "\n",
        "\n",
        "## Build your model\n",
        "\n",
        "It's time to implement your own models to get the best clasification performance on MNIST.\n",
        "First, try to overfit scratch_loader, for larger and larger sizes. Once you succeed, replace it by the real loaders and classify these digits!\n",
        "You may consider the following ideas, ranked by relevance:\n",
        "\n",
        "* batch norm\n",
        "* more layers\n",
        "* skip connections\n",
        "* dropout (on the high level features)\n",
        "* data augmentation with `transforms.RandomRotation`or `transforms.RandomAffine` at the dataset creation time\n",
        "\n",
        "If you need some help to understand padding, stride etc..., [here](https://github.com/vdumoulin/conv_arithmetic) is very good resource from a Mila alumni.\n",
        "\n",
        "You can use the cell below to compare the learning curves of your models. Don't forget to change the `'name'`value in the `results`dictionary between each try.\n",
        "\n",
        "## Fabriquez votre modèle\n",
        "\n",
        "Vous devez maintenant implémenter votre propre modèle. Essayez d'abord de mémoriser scratch_loader. Ensuite essayez de bien classifier MNIST. Vous pouvez considérer les idées suivantes classées selon la préférence de l'auteur ce ces lignes:\n",
        "\n",
        "* normalisation par lot \n",
        "* réseau plus profond\n",
        "* connections sautées\n",
        "* abandon de neurones\n",
        "* augmentation de données avec les méthodes `transforms.RandomRotation`ou `transforms.RandomAffine`  lors de la création du jeux de données.\n",
        "\n",
        "Si vous ne connaissez pas ces notions, c'est normal. Regardez la version anglaise. \n",
        "Si vous avez voulez mieux comprendre le fonctionnement des couches convolutionnelles, regardez [ces merveilleux gifs](https://github.com/vdumoulin/conv_arithmetic) réalisés par un ancien du Mila.\n",
        "\n",
        "Vous pouvez utiliser la cellule ci-dessous pour comparer vos courbes d'apprentissages entre elles. N'oubliez pas de changer  la valeur `'name'` dans le dictionnaire `results`  entre chaque essais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etp_txmWe3dg",
        "colab_type": "code",
        "outputId": "9602b9bc-3eaf-46b9-c162-42cfb9d25d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "# PLOTTING\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'])\n",
        "            ax1.set_ylabel('cross entropy')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "            ax2.plot(results['accuracy'], label = filename[:-4])\n",
        "            ax2.set_ylabel('accuracy')\n",
        "            ax2.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f87697f78d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAEGCAYAAAAzJXKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJLWI4U2SJKlFDG+SJEktYniTJElqkf8PSEeqrMoDek8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHMhlY9re3dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AwesomeNet(nn.Module):\n",
        "    \"\"\"The MNIST killer net.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(28*28, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc(x.view(-1, 28*28))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkfMFV1-e3dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAINING\n",
        "model = AwesomeNet()\n",
        "\n",
        "lr = 0.005\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "results = {'name':'awesome', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(model, scratch_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, scratch_loader)\n",
        "    \n",
        "    # save results\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRtZ1SnTe3dz",
        "colab_type": "text"
      },
      "source": [
        "If you want to learn more about Pytorch, here is a very comprehensive [tutorial](https://nbviewer.jupyter.org/github/ds4dm/tipsntricks/blob/master/pytorch/tutorial.ipynb) made by Mila for Mila. You are encouraged to look at it **after** this lab session.\n",
        "\n",
        "/\n",
        "\n",
        "Si vous voulez en apprendre plus sur PyTorch, il y a un [tutoriel](https://nbviewer.jupyter.org/github/ds4dm/tipsntricks/blob/master/pytorch/tutorial.ipynb) très comprehensif fait par le Mila pour le Mila. Vous êtes encouragé à le regarder **après** ce labo."
      ]
    }
  ]
}