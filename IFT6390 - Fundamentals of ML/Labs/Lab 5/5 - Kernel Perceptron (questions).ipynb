{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [py3k]","language":"python","name":"Python [py3k]"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"5 - Kernel Perceptron (questions).ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"qvfR-ZKwQL4E","colab_type":"text"},"source":["# Lab 5/Labo 5\n","\n","This lab consists of 2 parts:\n","* Understanding linear and kernel perceptrons\n","* Implementation of linear and kernel perceptrons\n"," \n"," <hr>\n"," \n","Ce lab consiste en 2 parties:\n","* Comprendre le perceptron linéaire et le perceptron à noyau\n","* Implémenter le perceptron linéaire et le perceptron à noyau"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Owaegu1ZQL4K","colab_type":"text"},"source":["## Perceptron and Stochastic Gradient Descent/Perceptron et descente du gradient stochastique\n","\n","We have studied that given a model and a cost function, we learn the parameters of the model by using stochastic gradient descent. This consists in iteratively modifying the parameters by looping on the examples $\\left(x, y\\right)$ and by adding a gradient term.\n","\n","<hr>\n","\n","Nous avons étudié que considérant un modèle et une fonction de coût, il est possible d'apprendre les paramètres d'un modèle avec la descente du gradient stochastique. Ceci consiste à mettre à jour de manière itérative les paramètres en passant au travers tous les exemples $\\left(x, y\\right)$ et en leur ajoutant le gradient calculé.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bp6XWQn2fFMt","colab_type":"text"},"source":["### Perceptron Algorithm/L'algorithme perceptron\n","\n","The perceptron algorithm is a stochastic learning algorithm that iteratively improves a model by running it on training samples, then updating the model whenever it finds it has made an incorrect classification. The model learned by the standard perceptron algorithm is a linear classifier: a vector of weights $\\mathbf{w}$ that is used to classify a sample vector $\\mathbf{x}$ as class $1$ or class $-1$ according to\n","\n","$${{\\hat {y}}=\\operatorname {sgn} (\\mathbf {w} ^{\\top }\\mathbf {x} )}$$\n","where a zero is mapped to minus one. (The \"hat\" on $\\hat{y}$ denotes an estimated value.)\n","\n","In pseudocode, the perceptron algorithm is given by:\n","\n","* Initialize $\\mathbf{w}$ to an all-zero vector of length $p$, the number of predictors (features).\n","* For some fixed number of iterations, or until some stopping criterion is met:\n","    - For each training example $\\mathbf{x}_i$ with ground truth label $y_i ∈ \\{-1, 1\\}$:\n","\n","    Let, $\\hat{y} = sgn(\\mathbf{w}^T \\mathbf{x}_i)$\n","    \n","    If $ \\hat{y} \\neq y_i$, $\\mathbf{w} := \\mathbf{w} + y_i \\mathbf{x}_i$\n","    \n","<hr>\n","\n","Le perceptron est un algorithme stochastique qui apprend, à l'aide d'exemples de d'entraînement, à mieux classifier en mettant à jour les paramètres dès qu'une erreur de classification est commise. L'algorithme perceptron standard est un classifieur linéaire. Plus précisément, il apprend un vecteur de poids $\\mathbf{w}$ qui est utilisé pour classifier un vecteur $\\mathbf{x}$ comme appartenant à la classe $1$ ou $-1$ selon \n","\n","$${{\\hat {y}}=\\operatorname {sgn} (\\mathbf {w} ^{\\top }\\mathbf {x} )}$$\n","où un zéro sera attribué à la classe $-1$.\n","\n","En pseudo-code, l'algorithme du perceptron opère comme suit:\n","\n","* Initialise $\\mathbf{w}$ en un vecteur de zéros de dimension $p$, où $p$ est le nombre de traits.\n","* Pour un nombre fixe d'itérations ou jusqu'à ce qu'un critère soit rencontré:\n","    - Pour chaque exemple d'entraînement $\\mathbf{x}_i$ avec la vraie classe $y_i ∈ \\{-1, 1\\}$:\n","\n","    Soit, $\\hat{y} = sgn(\\mathbf{w}^T \\mathbf{x}_i)$\n","    \n","    Si $ \\hat{y} \\neq y_i$, $\\mathbf{w} := \\mathbf{w} + y_i \\mathbf{x}_i$"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"xcSO21cIQL4L","colab_type":"text"},"source":["## Part 2: Non-Linear Transformations/Partie 2: transformations non-linéaires"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"duyGi5ECQL4M","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_jGyuyE0PPq","colab_type":"code","outputId":"af7c6e7b-4f9a-4ffc-f3b6-5952dfa2e4f1","executionInfo":{"status":"ok","timestamp":1571242213791,"user_tz":240,"elapsed":3548,"user":{"displayName":"Rémi Le Priol","photoUrl":"","userId":"06369136232577523795"}},"colab":{"base_uri":"https://localhost:8080/","height":622}},"source":["! rm cercle.txt ellipse.txt\n","! wget https://drive.google.com/open?id=1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i\n","#! wget https://www.dropbox.com/s/chfkj30rlnez38o/cercle.txt\n","! wget https://www.dropbox.com/s/u0jxsd8a0iuiqgx/ellipse.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-10-16 16:10:11--  https://drive.google.com/open?id=1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i\n","Resolving drive.google.com (drive.google.com)... 172.217.203.101, 172.217.203.138, 172.217.203.113, ...\n","Connecting to drive.google.com (drive.google.com)|172.217.203.101|:443... connected.\n","HTTP request sent, awaiting response... 307 OK\n","Location: https://drive.google.com/file/d/1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i/view?usp=drive_open [following]\n","--2019-10-16 16:10:11--  https://drive.google.com/file/d/1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i/view?usp=drive_open\n","Reusing existing connection to drive.google.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘open?id=1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i’\n","\n","\r          open?id=1     [<=>                 ]       0  --.-KB/s               \ropen?id=1UNh_lzZdW9     [ <=>                ]  66.37K  --.-KB/s    in 0.002s  \n","\n","2019-10-16 16:10:11 (30.1 MB/s) - ‘open?id=1UNh_lzZdW9ePeyDvpGdcw0QURk23Ny3i’ saved [67966]\n","\n","--2019-10-16 16:10:12--  https://www.dropbox.com/s/u0jxsd8a0iuiqgx/ellipse.txt\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/u0jxsd8a0iuiqgx/ellipse.txt [following]\n","--2019-10-16 16:10:12--  https://www.dropbox.com/s/raw/u0jxsd8a0iuiqgx/ellipse.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com/cd/0/inline/AqgcVR9wY0834r3aAvi5MQqbqIVEVvaT_A4QRoZQia2B1nghT3g2ltiOBO-hKOs6zxIT9tgpYPVRzTNUJwv8UxDoSmi5UTy5OGhNC9lZn1rJIQZHK1_N009Jy1tvVkCaAyk/file# [following]\n","--2019-10-16 16:10:12--  https://uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com/cd/0/inline/AqgcVR9wY0834r3aAvi5MQqbqIVEVvaT_A4QRoZQia2B1nghT3g2ltiOBO-hKOs6zxIT9tgpYPVRzTNUJwv8UxDoSmi5UTy5OGhNC9lZn1rJIQZHK1_N009Jy1tvVkCaAyk/file\n","Resolving uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com (uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n","Connecting to uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com (uc49eb69fe465238d44d73717dcd.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 42310 (41K) [text/plain]\n","Saving to: ‘ellipse.txt’\n","\n","ellipse.txt         100%[===================>]  41.32K  --.-KB/s    in 0.02s   \n","\n","2019-10-16 16:10:13 (2.29 MB/s) - ‘ellipse.txt’ saved [42310/42310]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hTHOu-ODQL4R","colab_type":"code","outputId":"3b270fb4-73a7-475c-e2a7-d7ee0dce2cfc","executionInfo":{"status":"error","timestamp":1571242229072,"user_tz":240,"elapsed":394,"user":{"displayName":"Rémi Le Priol","photoUrl":"","userId":"06369136232577523795"}},"colab":{"base_uri":"https://localhost:8080/","height":379}},"source":["# We start by loading the dataset\n","# Nous commencons par télécharger le dataset\n","\n","data = np.loadtxt('cercle.txt')\n","# data = np.loadtxt('ellipse.txt')\n","\n","# There are only 2 dimensions...\n","# Il n'y a que 2 dimensions...\n","train_cols = [0, 1]\n","# A variable to hold the index of the label column.\n","# On définit l'index de la colonne représentant les classes.\n","target_ind = [data.shape[1] - 1]\n","\n","n_classes = 2\n","n_train = 1500\n","\n","# Comment to get non-deterministic results\n","# Mettre en commentaire pour avoir des résultats non-déterministes\n","random.seed(3395)\n","\n","# Randomly select indices for the training and test set\n","# On sélectionne aléatoirement les indices and les train et test set\n","inds = np.arange(data.shape[0])\n","random.shuffle(inds)\n","train_inds = inds[:n_train]\n","test_inds = inds[n_train:]\n","    \n","# Separate the dataset into two sets: training and test.\n","# On sépare le dataset en training et test set.\n","train_set = data[train_inds,:]\t\n","train_set = train_set[:, train_cols + target_ind] \n","test_set = data[test_inds,:]\n","test_set = test_set[:, train_cols + target_ind]\n","\n","# Separate the test set: inputs and labels.\n","# On sépare le test set en entrées et en étiquettes\n","test_inputs = test_set[:,:-1]\n","test_labels = test_set[:,-1]\n","\n","# Function to plot the decision boundary\n","# Fonction pour tracer la frontière de décision\n","def plot(model, outputs):\n","    train_classes_pred = np.sign(outputs)\n","    plt.scatter(train_set[:, 0], train_set[:, 1], c=train_classes_pred)\n","    plt.scatter(test_set[:, 0], test_set[:, 1], c=classes_pred)\n","    plt.title('Prediction')\n","    plt.show()\n","\n","    plt.scatter(train_set[:, 0], train_set[:, 1], c=train_set[:,-1])\n","    plt.scatter(test_set[:, 0], test_set[:, 1], c=test_labels)\n","    plt.title('True labels')\n","    plt.show()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d39fb776bc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cercle.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# data = np.loadtxt('ellipse.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# There are only 2 dimensions...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    622\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: cercle.txt not found."]}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"25gFj7lEQL4U","colab_type":"text"},"source":["To demonstrate the use of non-linear transformations, we will make use of the circle and ellipse datasets, available on the website. We will first apply a transformation on the dataset before training. The algorithm will then uniquely work with the transformed data. To keep things simple at first, we will simply implement a transformation consisting of a degree two polynomial. And to make it even more simple, the circle and ellipse datasets are only in 2-d...\n","\n","$$\\phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$$\n","\n","$$\\phi(x_1, x_2) = \\left(1,x_1,x_2,x_1^2,x_2^2,x_1x_2\\right)$$\n","\n","**Exercise: implement this transformation in the polynomial function below:**\n","\n","<hr>\n","\n","Afin de démontrer l'utilité des transformations non-linéaires, nous allons faire usage des jeux de données *circle* et *ellipse*, disponible sur le lien fourni. Nous appliquerons une transformation sur le jeu de données avant d'entraîner nos modèles. L'algorithme sera ainsi entrâiner uniquement avec les données transformées. Pour simplifier les choses de prime abord, nous implémenterons une transformation polynomiale de degré 2. De plus, les jeux de données *circle* et *ellipse* n'ont que 2 dimensions. \n","\n","$$\\phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^6$$\n","\n","$$\\phi(x_1, x_2) = \\left(1,x_1,x_2,x_1^2,x_2^2,x_1x_2\\right)$$\n","\n","**Exercice: implémentez cette transformation à l'intérieur de la fonction polynomiale ci-dessous:**"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"p6Ko299MQL4U","colab_type":"code","colab":{}},"source":["# Takes a matrix of data examples x (n datapoints, input space dimension) as input (without labels) and returns the transformed\n","# matrix (n datapoints, projected space dimension)\n","# Cette fonction prend une matrice d'exemples x de dimension (n nombre de points, nombre de traits) comme entrée (sans les étiquettes)\n","# et retourne la matrice transformée de dimension (n nombre de points, dimension de l'espace projeté)\n","def polynomial(X):\n","    Y = np.zeros((X.shape[0], 6))\n","    Y[:, 0] = 1.\n","    # Please fill in the code below/complétez le code ci-dessous\n","    Y[:, 1] = ...\n","    Y[:, 2] = ...\n","    Y[:, 3] = ...\n","    Y[:, 4] = ...\n","    Y[:, 5] = ...\n","    return Y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"PyfcGj6BQL4X","colab_type":"text"},"source":["**Exercise: Now that the transformation is implemented, you should complete the `train` method inside the Perceptron class:**\n","\n","<hr>\n","\n","**Exercice: Maintenant que la transformation est effectuée, complétez la méthode `train` de la classe Perceptron:**"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"MczF5teQQL4Y","colab_type":"code","colab":{}},"source":["class Perceptron:\n","    def __init__(self):\n","        return\n","\n","    def train(self, train_data):\n","        n_example = train_data.shape[0]            \n","        self.weights = np.random.random(train_data.shape[1] - 1)\n","\n","        i = 0\n","        count = 0 # We stop when the set is linearly separated/On arrête lorsque le dataset est séparé linéairement\n","        n_iter = 0\n","        n_iter_max = n_example * 100\n","        while (count < n_example and n_iter < n_iter_max):\n","            # Please fill in the code below/Complétez le code ci-dessous\n","            if ...:\n","                self.weights = ...\n","                count = 0\n","            else:\n","                count = count + 1\n","            i = (i + 1) % n_example\n","            n_iter += 1\n","\n","    def compute_predictions(self, test_data):\n","        outputs = []\n","        for data in test_data:\n","            outputs.append(np.dot(data, self.weights))\n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"7YsPtlVcQL4a","colab_type":"code","outputId":"c9edaf0f-6c35-42c7-c1a8-751fa5d6e9c9","executionInfo":{"status":"error","timestamp":1571241932344,"user_tz":240,"elapsed":274,"user":{"displayName":"Rémi Le Priol","photoUrl":"","userId":"06369136232577523795"}},"colab":{"base_uri":"https://localhost:8080/","height":373}},"source":["print(\"We will train a perceptron on \", n_train, \" training examples\")\n","\n","# Transform the dataset\n","# On transforme le jeu de données\n","transformed_train_set = np.concatenate((polynomial(train_set[:,:-1]), train_set[:, -1][:, None]), axis=1)\n","transformed_test_inputs = polynomial(test_inputs)\n","\n","# Create and train the model\n","# On crée une instance et l'on entraîne le modèle\n","model = Perceptron()\n","model.train(transformed_train_set)\n","\n","# Obtain the ouputs on the test set.\n","# On obtient les sorties du test set\n","outputs = model.compute_predictions(transformed_test_inputs)\n","\n","# Convert the outputs into classes by taking the sign.\n","# On convertit les sorties en classes en effectuant la fonction signe \n","classes_pred = np.sign(outputs)\n","\n","# Measure the performance\n","# On mesure la performance\n","err = 1.0 - np.mean(test_labels==classes_pred)\n","print(\"The test error is \", 100.0 * err,\"%\")\n","\n","# Trace the decision boundary\n","# On trace la frontière de décision\n","outputs = model.compute_predictions(transformed_train_set[:,:-1])\n","plot(model, outputs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["We will train a perceptron on  1500  training examples\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c2307d46d524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Transform the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformed_train_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtransformed_test_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-ef2c9f13b399>\u001b[0m in \u001b[0;36mpolynomial\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Please fill in the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ellipsis'"]}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NVxwZLDOQL4d","colab_type":"text"},"source":["## 2. Kernel trick/L'astuce du noyau\n","\n","Your objective for this section is to implement a gaussian kernel perceptron. We will ask you to use a gaussian kernel for this problem. For a review, please refer to the [course document](https://studium.umontreal.ca/pluginfile.php/4043631/mod_resource/content/2/13_kernel_trick-en-3395.pdf).\n","\n","**Exercise: implement a polynomial kernel function of arbitrary degree**\n","\n","<hr>\n","\n","Votre but dans cette section est d'implémenter un noyau gaussien pour le perceptron. Nous allons utilisé le noyau Gaussien pour ce problème. Pour une révision de l'astuce du noyau, référez-vous aux [notes de cours](https://studium.umontreal.ca/pluginfile.php/5052299/mod_resource/content/1/13_kernel_trick.pdf).\n","\n","**Exercice: implémentez une fonction de noyau polynomiale de degré arbitraire**"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"rbtKg62KQL4e","colab_type":"code","colab":{}},"source":["def kernel_polynomial(x, y, deg=2):\n","    # https://en.wikipedia.org/wiki/Polynomial_kernel\n","    return ...\n","\n","def kernel_rbf(x, y):\n","    return ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkpf9l_FQwjm","colab_type":"text"},"source":["### Kernel methods\n","\n","A *kernel machine* is a classifier that stores a subset of its training examples $\\mathbf{x}_i$, associates with each a weight $α_i$, and makes decisions for new samples $\\mathbf{x}'$ by evaluating\n","\n","$${\\operatorname{sgn} \\sum _{i=1}^N\\alpha _{i}y_{i}K(\\mathbf {x} _{i},\\mathbf {x'} )}$$\n","\n","Here, $K$ is some kernel function. Formally, a kernel function is a non-negative semidefinite kernel, representing an inner product between samples in a high-dimensional space, as if the samples had been expanded to include additional features by a function $\\phi: K(\\mathbf{x}, \\mathbf{x'}) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}')$. Intuitively, it can be thought of as a similarity function between samples.\n","\n","### Kernel Perceptron\n","\n","To derive the kernel perceptron algorithm, we must first formulate it in dual form, starting from the observation that the weight vector $\\mathbf{w}$ can be expressed as a linear combination of the $n$ training samples. The equation for the weight vector is\n","$$\\mathbf{w} =\\sum _{i}^{n}\\alpha _{i}y_{i}\\mathbf {x} _{i}$$\n","where $\\alpha_i$ is the number of times $\\mathbf{x}_i$ was misclassified, forcing an update $\\mathbf{w} := \\mathbf{w} + y_i x_i$. Using this result, we can formulate the dual perceptron algorithm, which loops through the samples as before, making predictions, but instead of storing and updating a weight vector $\\mathbf{w}$, it updates a *mistake counter* vector $\\mathbf{\\alpha}$. We must also rewrite the prediction formula to get rid of $\\mathbf{w}$:\n","\n","$${\\begin{aligned}{\\hat{y}}&=\\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} )\\\\&=\\operatorname {sgn} \\left(\\sum _{i}^{n}\\alpha _{i}y_{i}\\mathbf {x} _{i}\\right)^{\\mathsf {T}}\\mathbf {x} \\\\&=\\operatorname {sgn} \\sum _{i}^{n}\\alpha _{i}y_{i}(\\mathbf {x} _{i}\\cdot \\mathbf {x} )\\end{aligned}}$$\n","\n","\n","Plugging these two equations into the training loop turn it into the dual perceptron algorithm.\n","\n","Finally, we can replace the dot product in the dual perceptron by an arbitrary kernel function, to get the effect of a feature map $\\phi$ without computing $\\phi(x)$ explicitly for any samples. Doing this yields the kernel perceptron algorithm:\n","\n","* Initialize $\\alpha$ to an all-zeros vector of length $n$, the number of training samples.\n","* For some fixed number of iterations, or until some stopping criterion is met:\n","    For each training example $x_j$, $y_j$:\n","    Let $${\\hat{y}}=\\operatorname {sgn} \\sum _{i}^{n}\\alpha _{i}y_{i}K(\\mathbf {x} _{i},\\mathbf {x} _{j})$$\n","    If $\\hat{y} \\neq y_j$, perform an update by incrementing the mistake counter:\n","    $$\\alpha_j := \\alpha_j + 1$$\n","    \n","<hr>\n","\n","### Les méthodes à noyau\n","\n","Une machine à noyau est un classifieur qui enregistre une partie des exemples d'entraînement $\\mathbf{x}_i$, qui associe ces exemples à un poids $α_i$ et qui prend une décision pour de nouveaux exemples en évaluant:\n","\n","$${\\operatorname{sgn} \\sum _{i=1}^N\\alpha _{i}y_{i}K(\\mathbf {x} _{i},\\mathbf {x'} )}$$\n","\n","Ici, $K$ est une fonction de noyau quelconque. Formellement, une fonction de noyau est semi-définie positive, représentant un produit scalaire dans l'espace à haute dimension où les exemples sont *mappés*. En d'autres mots, les exemples sont *mappés* vers un espace à plus haute dimension par la fonction $\\phi: K(\\mathbf{x}, \\mathbf{x'}) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}')$, de manière à ce que ces exemples contiennent de nouveaux traits. Intuitivement, ceci peut être vu comme étant une fonction de ressemblance entre les exemples.\n","\n","### Le perceptron à noyau\n","\n","Afin de dériver le perceptron à noyau, nous devons premièrement le formuler dans ses deux formes, en commencant par exprimer le vecteur de poids $\\mathbf{w}$ par une combinaison linéaire des $n$ exemples d'entraînement. L'expression des poids $\\mathbf{w}$ se veut comme suit:\n","\n","$$\\mathbf{w} =\\sum _{i}^{n}\\alpha _{i}y_{i}\\mathbf {x} _{i}$$\n","où $\\alpha_i$ est le nombre de fois que $\\mathbf{x}_i$ a été mal classé, imposant une mise à jour $\\mathbf{w} := \\mathbf{w} + y_i x_i$. En utilisant cette équation, nous pouvons maintenant reformuler l'algorithme du preceptron, qui passent à travers les exemples, comme précédemment, en effectuant des predictions mais, plutôt que d'emmagasiner et de mettre à jour le vecteur de poids $\\mathbf{w}$, nous mettrons à jour le *compteur* d'erreur $\\mathbf{\\alpha}$. Ainsi, pour se débarasser du terme $\\mathbf{w}$, nous réécrivons la formule de prédiction comme suit:\n","\n","$${\\begin{aligned}{\\hat{y}}&=\\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} )\\\\&=\\operatorname {sgn} \\left(\\sum _{i}^{n}\\alpha _{i}y_{i}\\mathbf {x} _{i}\\right)^{\\mathsf {T}}\\mathbf {x} \\\\&=\\operatorname {sgn} \\sum _{i}^{n}\\alpha _{i}y_{i}(\\mathbf {x} _{i}\\cdot \\mathbf {x} )\\end{aligned}}$$\n","\n","Insérez ces deux expressions dans la méthode d'entraînement résulte à l'algorithme du perceptron duel.\n","\n","Finalement, nous pouvons remplacer le produit scalaire par une fonction de noyau quelconque afin d'avoir un *mapping* $\\phi$ sans avoir à explicitement calculer la transformation $\\phi(x)$ pour les exemples. Effectuer cette boucle est par définition l'algorithme du perceptron à noyau:  \n","\n","* Initialise $\\alpha$ par un vecteur de zéros de dimension $n$, soit le nombre d'exemples d'entraînement.\n","* Pour un nombre fixe d'itérations ou jusqu'à ce qu'un critère soit rempli:\n","    Pour chaque exemple d'entraînement $x_j$, $y_j$:\n","    Soit $${\\hat{y}}=\\operatorname {sgn} \\sum _{i}^{n}\\alpha _{i}y_{i}K(\\mathbf {x} _{i},\\mathbf {x} _{j})$$\n","    Si $\\hat{y} \\neq y_j$, effectue une mise à jour en ajoutant 1 au *compteur* d'erreurs:\n","    $$\\alpha_j := \\alpha_j + 1$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"aAAw1MBkQL4g","colab_type":"text"},"source":["Now, we will implement the kernel perceptron.\n","\n","**Exercise: Complete the code below for the kernel perceptron algorithmn**\n","\n","Hint: Reuse the perceptron code above, but rather than learning the vector $w$ directly, you need to rewrite $\\mathbf{w} = \\sum_i \\alpha_i \\mathbf{x}_i$ and to learn the coefficients $a_i$. Then, we replace $\\mathbf{w}^T x$ by $\\left(\\sum_i \\alpha_i \\mathbf{x}_i\\right)^T x$ and apply the kernel trick.\n","\n","<hr>\n","\n","Maintenant, nous allons implémenter le perceptron à noyau.\n","\n","**Exercice: Complétez le code suivant pour l'algorithme du perceptron avec noyau**\n","\n","Indication: Repartez du code du perceptron ci-dessus, mais plutôt que d'apprendre le vecteur $w$ directement, il faut le réécrire $w = \\sum_i \\alpha_i x_i$ et apprendre les coefficients $a_i$. Ensuite, on remplace $w^T x$ par $\\left(\\sum_i \\alpha_i x_i\\right)^T x$ puis, on applique l'astuce du noyau."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"bTTtqNEmQL4h","colab_type":"code","colab":{}},"source":["class KernelPerceptron:\n","    def __init__(self, kernel_fn):\n","        self.kernel_fn = kernel_fn\n","\n","    def train(self, train_data):\n","        n_example = train_data.shape[0]\n","        self.train_data = np.array(train_data)\n","\n","        # alpha initialization\n","        # initialisation d'alpha\n","        self.a = np.zeros(n_example)\n","\n","        # Get rid of the labels\n","        # On enlève les étiquettes\n","        train_x = self.train_data[:,:-1]\n","        train_y = self.train_data[:,-1]\n","\n","        self.train_x = train_x\n","        self.train_y = train_y\n","\n","        # Gram matrix\n","        K = np.zeros((n_example, n_example))\n","        for i in range(n_example):\n","            K[i] = self.kernel_fn(train_x[i], train_x)\n","\n","        # Kernel calculation\n","        # Calcul du noyau\n","        i = 0\n","        count = 0\n","        n_iter = 0\n","        n_iter_max = n_example * 100\n","        while (count < n_example and n_iter < n_iter_max):\n","            # Please write in the following code/écrivez votre code ici\n","            if ...:\n","                ...\n","                count = 0\n","            else:\n","                count = count + 1\n","            i = (i + 1) % n_example\n","            n_iter += 1\n","\n","    def compute_predictions(self, test_data):\n","        outputs = []\n","        for i in range(len(test_data)):\n","            # Please write in the following code/écrivez votre code ici\n","            prediction = ...\n","            outputs.append(prediction)\n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"KtnqzrOmQL4j","colab_type":"code","colab":{}},"source":["print(\"We will learn a kernel perceptron on \", n_train, \" training examples\")\n","\n","model = KernelPerceptron(kernel_polynomial)\n","model.train(train_set)\n","\n","outputs = model.compute_predictions(test_inputs)\n","classes_pred = np.sign(outputs)\n","err = 1.0 - np.mean(test_labels==classes_pred)\n","print(\"The test error is {}%\".format(100.0 * err))\n","outputs = model.compute_predictions(train_set[:,:-1])\n","\n","plot(model, outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Pwe0srH4QL4m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}