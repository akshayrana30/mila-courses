{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"name":"IFT6390_Lab1_K-NN EN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TxmxfpdhG42L","colab_type":"text"},"source":["# A little theory on kNN / Un peu de théorie sur k-PPV"]},{"cell_type":"markdown","metadata":{"id":"A1BBCkJoG42O","colab_type":"text"},"source":["## Intuition / Intuition"]},{"cell_type":"markdown","metadata":{"id":"NFm85oIiG42P","colab_type":"text"},"source":["The algorithm $k$ Nearest Neighbours is one of the simpler machine learning algorithms. It is motivated by the idea that similar examples $x_t$ should have similar targets $y_t$.\n","\n","So, to predict the target class of a test examples $x$, all we need to do is find the $k$ nearest neighbours to $x$ using some metric - for example, euclidean distance also known as $L_2$ norm, or more generally minkowski distance $L_p$.\n","\n","In a classification problem, we use those $k$ nearest neighbours to predict the target class of $x$ as the most common target of its neighbours i.e. it's as though each neighbour of $x$ casts a vote for their own target class and the class with the most votes wins.\n","\n","<hr/>\n","\n","L'algorithme des $k$ plus proches voisins ($k$-PPV) est certainement un des algorithmes les plus simples d'apprentissage automatique. Il est motivé par l'idée que des *entrées* $x_t$ semblables devraient avoir des *cibles* $y_t$ semblables. Ainsi, pour bien définir un algorithme $k$-PPV, il suffit de définir ce que veut dire *semblable* dans le contexte des entrées et de définir l'influence de ces voisins sur la prédiction de la cible pour une entrée de test.\n","\n","Donc, pour obtenir une prédiction de la cible pour une entrée de test $x$, il suffit de trouver les k plus proches voisins selon une métrique déterminant jusqu'à quel point des entrées sont semblables (par exemple, la distance euclédienne ou norme $L^2$, ou de façon plus générale la norme $L^p$ de Minkowski) et d'utiliser ces $k$ plus proches voisins pour prédire la cible de $x$. \n","\n","Dans un problème de classification, la prédiction correspond à la classe majoritaire parmi les $k$ plus proches voisins, i.e. que l'ensemble des $k$ plus proches voisins votent pour la classe correspondant à leur cible respective et la classe recueillant le plus de vote est choisie en tant que prédiction par l'algorithme."]},{"cell_type":"markdown","metadata":{"id":"YJAsbLyPG42W","colab_type":"text"},"source":["## Mathematic Formalism / Formalisation mathématique"]},{"cell_type":"markdown","metadata":{"id":"jJ6RVf7tG42Y","colab_type":"text"},"source":["Let\n","\n","* $x$ be test examples\n","* $m$ be the number of classes\n","* $D_n = \\{(x_i,y_i)\\}_{i=1}^n$ be the training data where $y_i \\in Y=\\{1,\\dots,m\\}$ is the corresponding target class of example $x_i$\n","* $d(\\dot{},\\dot{})$ be our distance metric\n","* $V(x, k)$ is the set of $k$ nearest neighbours of $x$\n","\n","<br/>\n","\n","The prediction by the $k$-NN algorithm is therefore:\n","\n","> $$f(x)={\\mbox{arg max}} \\left(\\frac{1}{k} \\sum_{(x_i,y_i) \\in V(x, k)} \\mathrm{onehot}_{m}(y_i)\\right)$$\n"]},{"cell_type":"markdown","metadata":{"id":"3fWJjTPHVwee","colab_type":"text"},"source":["To find out the $k$ nearest neighbours, we need to calculate some \"distance\" between the test sample $x$ and all the samples in the training set, and pick the $k$ samples with the least distance from $x$.\n"," \n","A common distance function is the euclidean distance function:\n","> $$d(a,b)= \\sqrt{\\sum_{i=1}^{dim}(a_i-b_i)^2}$$\n","\n","which is a specific case of the $L_p$ norm of Minkowski (where $p = 2$)\n","> $$d(a,b)= \\left(\\sum_{i=1}^d|a_i-b_i|^p\\right)^\\frac{1}{p}$$\n","\n","<hr/>"]},{"cell_type":"markdown","metadata":{"id":"1anrnZqFG42a","colab_type":"text"},"source":["Soit\n","\n","* $x$ une entrée de test\n","* $m$ le nombre de classes\n","* $D_n = \\{(x_i,y_i)\\}_{t=1}^n$ l'ensemble d'entraînement, où $y_i \\in Y=\\{1,\\dots,m\\}$ correspond à l'identité de la classe cible de l'entrée $x_i$\n","* $d(\\dot{},\\dot{})$ une fonction de distance\n","* $V(x,k)$ l'ensemble des $k$ plus proches voisins de $x$\n","\n","<br/>\n","\n","La prédiction par l'algorithme des k plus proches voisins est donc:\n","\n","> $$f(x)={\\mbox{arg max}} \\left(\\frac{1}{k} \\sum_{(x_i,y_i) \\in V(x, k)} \\mathrm{onehot}_{m}(y_i)\\right)$$\n","\n","Afin de déterminer les $k$ plus proches voisins, nous avons besoin de calculer une certaine métrique de distance entre les points test $x$ et les points du jeu d'entraînement. Nous déterminons ensuite les $k$ plus proches points du point text $x$.\n","\n","Une fonction communément utilisée est celle de la distance euclidenne:\n","> $$d(a,b)= \\sqrt{\\sum_{i=1}^{dim}(a_i-b_i)^2}$$\n","\n","qui est un cas spécifique de la norme $L^p$ de Minkowski(où $p = 2$)\n","> $$d(a,b)= \\left(\\sum_{i=1}^d|a_i-b_i|^p\\right)^\\frac{1}{p}$$"]},{"cell_type":"markdown","metadata":{"id":"Tp9Q6E5cG42c","colab_type":"text"},"source":["## Pseudocode"]},{"cell_type":"markdown","metadata":{"id":"OLhAaXiRG42d","colab_type":"text"},"source":["We define a machine learning algorithm by specifying its training procedure for some training data and how to predict the target for a test example. Given that the training procedure for $k$-NN is simply loading the training data $D_n$, we can specify how to predict the target class for the case when $k = 1$:\n","\n","    def kNN_1(x):\n","        min = +inf # intialize the distance of the nearest neighbour\n","        idx = -1 # initialize the index of the nearest neighbour\n","        \n","        for i=1 to n:\n","            dt = d(X[i], x)\n","            if dt < min\n","                min = dt\n","                idx = i\n","                \n","        return Y[idx]\n","\n","This runs in $O(n(k+d))$ time but you can get $O(n(log(k)+d))$ time by using a priority queue (heap)\n","\n","<hr/>\n","\n","On définit un algorithme d'apprentissage en précisant sa procédure d'entraînement et de prédiction pour une nouvelle entrée de test. Étant donné que la procédure d'entraînement de l'algorithme $k$-PPV consiste simplement à mettre en mémoire l'ensemble d'entraînement $D_n$, voici donc la procédure de prédiction:\n","\n","    def k-PPV(x):\n","        min = +inf # initialiser la distance du plus proche voisin\n","        idx = -1 # initialiser l'index du plus proche voisin\n","        \n","        for i=1 to n:\n","            dt = d(X[i], x)\n","            if dt < min\n","                min = dt\n","                idx = i\n","                \n","        return Y[idx]\n","        \n","Cette procédure de prédiction s'exécute en temps $O(n(k+d))$. Il est cependant possible d'obtenir un temps d'exécution dans $O(n(log(k)+d))$, en utilisant une queue de priorité (monceau)."]},{"cell_type":"markdown","metadata":{"id":"l9J6ddy0G42e","colab_type":"text"},"source":["# Putting it in practice! / Mise en pratique!"]},{"cell_type":"markdown","metadata":{"id":"E9JHG9QLG42f","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"UKBwCm8VG42h","colab_type":"text"},"source":["We want to make a machine learning algorithm to identify flowers. We have three types of iris species and we will try to use the characteristics of each flower (features) to determine which species of iris it is (class). But you don't know anything about flowers! So we will learn this algorithm using a dataset of flower measurements and the classes those flowers correspond to (training data), and we will use 1-kNN!\n","\n","<hr/>\n","\n","On vous demande de concevoir un algorithme d'apprentissage permettant d'identifier des fleurs. Il s'agit de trois variétés d'iris. C'est à partir de ces caractéristiques (traits) que vous devez déterminer la sorte de chaque fleur (la classe). Vous ne connaissez rien aux fleurs! Fort heureusement vous disposez d'un ensemble d'entraînement associant à divers exemples de mesures d'iris la bonne variété (classe). Armé de l'algo 1-PPV et de Python vous foncez tête baissée."]},{"cell_type":"markdown","metadata":{"id":"4UtjXydlG42h","colab_type":"text"},"source":["## 1. Calculate the $L^p$ (Minkowski) distance between two vectors / Calculez la distance $L^p$ (Minkowski) entre deux vecteurs"]},{"cell_type":"markdown","metadata":{"id":"aSlrdn_6G42j","colab_type":"text"},"source":["We want a function that given two vectors (`np.array`) will output the Minkowski distance between them. Complete the function `minkowski_vec` below. Test it yourself on two vectors (you can import the iris dataset as we did in the tutorial to use real iris vectors).\n","\n","### ***Ex: Calculate Minkowski distance between two vectors***. Here is the [definition](http://en.wikipedia.org/wiki/Minkowski_distance) in case you need it\n","\n","<hr/>\n","\n","Nou voulons définir une fonction qui en prenant 2 vecteurs retournera la distance Minkowski entre les deux. Completez la fonction `minkowski_vec` ci-dessous. Testez la fonction sur deux vecteurs (vous pouvez importez  le dataset iris et créez 2 vecteurs afin d'effectuer le test).\n","\n","### ***Ex: Calculez la distance Minkowski entre deux vecteurs***. Voici la définition, au besoin : [definition](http://en.wikipedia.org/wiki/Minkowski_distance)"]},{"cell_type":"code","metadata":{"id":"rBtUWejJG42k","colab_type":"code","colab":{}},"source":["import numpy as np\n","def minkowski_vec(x1, x2, p=2.0):\n","    dist = # insert code here /insérer votre code ici\n","    \n","    return dist\n","\n","# for testing\n","a = np.ones((4,))\n","b = np.zeros((4,))\n","print(minkowski_vec(a, b))\n","print(minkowski_vec(a, a))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6gLdLU9G42q","colab_type":"text"},"source":["**Note:** since this is a vector, we'll need to apply operations to the different dimensions. We could do this by iterating over the array one element at a time (e.g. here to calculate the difference in absolute values)\n","\n","    s = 0\n","    for i in range(x1.shape[0]):\n","        s = s + abs(x1[i] - x2[i])\n","\n","or we could use `numpy` intelligently to do the same thing:\n","\n","    s = np.sum(np.abs(x1 - x2))\n","\n","the difference is that the second option is not just more compact and easy to read, it uses `numpy`'s library to calculate the sums and operations which is much much faster than Python because they are specialized math functions written in C++. \n","\n","In short, use numpy functions instead of for loops where possible!\n","\n","<hr/>\n","\n","\n","**Note:** on peut calculer cette distance en itérant sur chaque composante de x1 et x2, et en calculant la somme après, ou on peut profiter du fait que la plupart (ou même toutes) des opérations mathématiques (abs, +, -, \\*\\* etc.) sur des structures de données itérables (listes, vecteurs/matrices) produisent des résultats équivalents à l'application d'une boucle for, mais en beaucoup moins de temps (on parle des langages interprétés comme Python). Par exemple, on peut calculer la somme de la différence des valeurs absolues de x1 et x2 comme suit:\n","\n","    s = 0\n","    for i in range(x1.shape[0]):\n","        s = s + abs(x1[i] - x2[i])\n","\n","ou on peut simplement utiliser `numpy` de manière intelligente comme suit:\n","\n","    s = numpy.sum(numpy.abs(x1 - x2))\n","\n","En plus d'être plus compacte, la deuxième option est beaucoup plus rapide (parce qu'elle fait appel à une implémentation efficace de sum et abs en C++).\n","\n","En résumé, utilisez les fonctions numpy à la place de boucle `for` le plus possible!"]},{"cell_type":"markdown","metadata":{"id":"XF27CGFvG42r","colab_type":"text"},"source":["## 2. Calculate $L^p$ distance between a vector and a matrix / Calculer une distance $L^p$ entre un vecteur et une matrice"]},{"cell_type":"markdown","metadata":{"id":"1soCeiIEG42r","colab_type":"text"},"source":["We also need a function to compare one flower to a bunch of other flowers. We will modify the function `minkowski_mat` to calculate an $L_p$ distance between a single vector (or 1D `np.array`) and a matrix (or 2D `np.array`). This function should return an array of distances corresponding to the distance between the single flower $x1$ and each other flower in the bunch represented as rows in $X2$.  Use `np.newaxis` or broadcasting rules.\n","\n","### ***Ex: Calculate Minkowski distance between a vector and a matrix***\n","\n","<hr/>\n","\n","\n","Il nous faut aussi une fonction qui va nous permettre de comparer une fleur avec tout un ensemble de fleurs, sur la base de leurs traits. On va maintenant modifier la fonction `minkowski_mat` pour calculer une *distance* $L^p$ entre un vecteur et une matrice, c.a.d. une fonction qui va nous retourner un vecteur de distances $L^p$ entre une fleur $x1$ et toutes les autres fleurs représentées par les lignes de la matrice $X2$. Utilisez `np.newaxis`.\n","\n","### ***Ex: Calculez la distance Minkowski entre un vecteur et une matrice***"]},{"cell_type":"code","metadata":{"id":"7HMkQ25MG42t","colab_type":"code","colab":{}},"source":["def minkowski_mat(x1, X2, p=2.0):\n","    dist = # insert code here/ insérer votre code ici\n","    \n","    return dist\n","\n","# for testing\n","a = np.ones((4,))\n","b = np.random.randint(5, size=(10,4))\n","print(minkowski_mat(a,b))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wYgZSK5G426","colab_type":"text"},"source":["Reiterating the very important point **the vast majority of vector-vector, vector-matrix, or matrix-matrix operations are going to be much more efficient in numpy than in python using a for loop**. \n","\n","You may have noticed that the difference in the efficient example implementations of `minkowski_vec` and `minkowski_mat` is just the part: `axis=1`. This exercise is to understand why it is important (and necessary) to specify on which axis you are applying the sum.\n","\n","<hr/>\n","\n","\n","Encore une fois, une chose importante à retenir: **la  grande majorité des opérations vecteur-vecteur, vecteur-matrice ou bien matrice-matrice seront beaucoup plus efficaces en utilisant les opérateurs numpy au lieu d'une boucle for.**\n"," \n","Vous avez peut-être remarqué que la différence entre les implémentations efficaces de `minkowski_vec` et `minkowski_mat` est seulement la partie: `axis=1`. L'exercice est de comprendre pourquoi il est nécessaire de spécifier sur quel *axe* on va faire la somme."]},{"cell_type":"markdown","metadata":{"id":"RBuDQBOhG427","colab_type":"text"},"source":["## 3. 1-KNN / 1-PPV"]},{"cell_type":"markdown","metadata":{"id":"Kux7hYpcG428","colab_type":"text"},"source":["We're nearly there!\n","\n","### ***Ex: Finish the following function to predict the species of a test data point with features `x`:***\n","- From `data`, slice the `features` and `target`\n","- Calculate the distance between `x` and the `features` using `minkowski_mat`\n","- Find the data point with the least distance, and return its `target`\n","\n","<hr/>\n","\n","Nous y sommes presque!\n","\n","### ***Ex: Complétez la fonction suivante afin de prédire l'espèce d'un point test avec ses traits `x` :***\n","- À partir de `data`, découpez les `features` et les `target`\n","- Calculez la distance entre les features `x` et les données d'entraînement en utilisant `minkowski_mat`\n","- Trouvez le point de donnée avec la distance minimale et retournez sa classe `target`\n"]},{"cell_type":"code","metadata":{"id":"Z3TvSv5cG42-","colab_type":"code","colab":{}},"source":["def knn(x, data, p=2): \n","    # insert code here/ insérer votre code ici\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCltbAmHG43E","colab_type":"text"},"source":["Note that `x` is the array of features (no target) of the test example. This functions should be quite easy to write then because `minkowski_mat` will output an array of distances, and we want to find the *index* of the *minimum* of those distances, then just find the target at that index.\n","\n","<hr/>\n","\n","À noter ici que `x` est le vecteur de caractéristiques (sans étiquette) de l'exemple de test. Ayant en mains la fonction `minkowski_mat`, les choses devraient être simples, car cette fonction retournera un vecteur/liste de distances. Il faut ensuite trouver l'exemple/la fleur qui est la plus *proche* (dans le sens de minkowski) de `x` et ainsi, on va conclure que l'étiquette (prédite) de `x` est celle de cet exemple."]},{"cell_type":"markdown","metadata":{"id":"-plNqvtQG43G","colab_type":"text"},"source":["## Conclusion / Conclusion"]},{"cell_type":"markdown","metadata":{"id":"hhuQMZ1rG43I","colab_type":"text"},"source":["We now have all the components of the 1-NN algorithm, all that's left is to put it all together and test it\n","\n","Remember that we can access the functions we've executed in the previous cells\n","\n","After testing your implementation, write a `for` loop which will call `knn(iris[i,:-1], iris, p)` for each example `i` and compare the prediction to the true target `iris[i,-1]`. The two should always be the same, why?\n","\n","<hr/>\n","\n","Nous avons maintenant en notre possession toutes les composantes de l'algorithme 1-PPV. Il reste simplement à en faire l'assemblage et à tester le tout. \n","\n","Rappelez-vous que les fonctions définies dans les cellules de code précédentes sont accessibles dans toutes les cellules subséquentes une fois les précédentes exécutées.\n","\n","Afin de tester votre implémentation, écrivez une boucle `for` qui appelle, pour chaque exemple `i`, la fonction `ppv(iris[i,:-1],iris,p)` et qui compare la classe prédite avec `iris[i,-1]` (la vrai étiquette). Les deux devraient toujours être les mêmes (pourquoi?)."]},{"cell_type":"code","metadata":{"id":"M-NH1XPDG43K","colab_type":"code","colab":{}},"source":["import numpy as np\n","iris = np.loadtxt('iris.txt')\n","\n","predictions = np.zeros(iris.shape[0])\n","for i in range(iris.shape[0]):\n","    predictions[i] = knn(iris[i,:-1],iris)\n","    \n","targets = iris[:,-1]\n","print(\"error rate:\",(1.0-(predictions==targets).mean())*100.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P74PpuHdG43U","colab_type":"text"},"source":["## Bonus and things to reflect on for the next time / Bonus et trucs auxquels réfléchir pour la prochaine fois"]},{"cell_type":"markdown","metadata":{"id":"jbwhn39uG43W","colab_type":"text"},"source":["In machine learning, we usually have a split of training data and testing data\n","* divide the dataset in two : one training dataset with 50 examples (randomly sampled) and a testing dataset with the remaining examples. \n","* Using the training data to find the nearest neighbours and target output for a given example, calculate the performance of your algorithm on training and testing. Why is there such a difference? \n","\n","<hr/>\n","\n","En apprentissage machine, nous divisons habituellement un jeu de données en un jeu d'entraînement et un jeu de données test\n","* divisez l'ensemble iris en deux - un ensemble d'entraînement qui contient 50 exemples (un sous-ensemble aléatoire!) et un ensemble de test qui contient le reste.\n","* utilisez le premier sous-ensemble comme données sur lesquelles on va calculer les distances minkowski (donc données d'entraînement).Calculez la performance de votre algorithme sur les deux ensembles. Pourquoi y a-t-il une telle différence? "]},{"cell_type":"code","metadata":{"id":"V4VXtbh7G43X","colab_type":"code","colab":{}},"source":["indexes = np.arange(iris.shape[0])\n","# set the random seed so we have exact reproducibility\n","np.random.seed(3395)\n","np.random.shuffle(indexes)\n","\n","train_set = iris[indexes[:50]]\n","test_set = iris[indexes[50:]]\n","\n","# predictions on the training set\n","train_predictions = np.zeros(train_set.shape[0])\n","for i in range(train_set.shape[0]):\n","    train_predictions[i] = knn(train_set[i,:-1],train_set)\n","    \n","# predictions on the testing set\n","test_predictions = np.zeros(test_set.shape[0])\n","for i in range(test_set.shape[0]):\n","    test_predictions[i] = knn(test_set[i,:-1],train_set)\n","    \n","print(\"Training data error rate\", (1.0-(train_predictions==train_set[:,-1]).mean())*100.0)\n","print(\"Testing data error rate\", (1.0-(test_predictions==test_set[:,-1]).mean())*100.0)"],"execution_count":0,"outputs":[]}]}